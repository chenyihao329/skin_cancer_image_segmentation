{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from functools import partial\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.signal\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mmcv\n",
    "# from mmcv.fileio import FileClient\n",
    "# # from mmcv.fileio import load as load_file\n",
    "# from mmcv.parallel import is_module_wrapper\n",
    "# from mmcv.utils import mkdir_or_exist\n",
    "# from mmcv.runner import get_dist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    \n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    # use _load_from_state_dict to enable checkpoint version control\n",
    "    def load(module, prefix=''):\n",
    "        # recursively check parallel module in case that the model has a\n",
    "        # complicated structure, e.g., nn.Module(nn.Module(DDP))\n",
    "#         if is_module_wrapper(module):\n",
    "#             module = module.module\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys,\n",
    "                                     err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None  # break load->load reference cycle\n",
    "\n",
    "    # ignore \"num_batches_tracked\" of BN layers\n",
    "    missing_keys = [\n",
    "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
    "    ]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append('unexpected key in source '\n",
    "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(\n",
    "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    rank = 0\n",
    "    if len(err_msg) > 0 and rank == 0:\n",
    "        err_msg.insert(\n",
    "            0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "\n",
    "\n",
    "def _load_checkpoint(filename, map_location=None):\n",
    "    checkpoint = torch.load(filename, map_location=map_location)\n",
    "    return checkpoint\n",
    "\n",
    "def load_checkpoint(model,\n",
    "                    filename,\n",
    "                    map_location='cpu',\n",
    "                    strict=False,\n",
    "                    logger=None):\n",
    "    \n",
    "    checkpoint = _load_checkpoint(filename, map_location)\n",
    "    # OrderedDict is a subclass of dict\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\n",
    "            f'No state_dict found in checkpoint file {filename}')\n",
    "    # get state_dict from checkpoint\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    # strip prefix of state_dict\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "\n",
    "    # for MoBY, load model of online branch\n",
    "    if sorted(list(state_dict.keys()))[0].startswith('encoder'):\n",
    "        state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}\n",
    "\n",
    "    # reshape absolute position embedding\n",
    "    if state_dict.get('absolute_pos_embed') is not None:\n",
    "        absolute_pos_embed = state_dict['absolute_pos_embed']\n",
    "        N1, L, C1 = absolute_pos_embed.size()\n",
    "        N2, C2, H, W = model.absolute_pos_embed.size()\n",
    "        if N1 != N2 or C1 != C2 or L != H*W:\n",
    "            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n",
    "        else:\n",
    "            state_dict['absolute_pos_embed'] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n",
    "\n",
    "    # interpolate position bias table if needed\n",
    "    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n",
    "    for table_key in relative_position_bias_table_keys:\n",
    "        table_pretrained = state_dict[table_key]\n",
    "        table_current = model.state_dict()[table_key]\n",
    "        L1, nH1 = table_pretrained.size()\n",
    "        L2, nH2 = table_current.size()\n",
    "        if nH1 != nH2:\n",
    "            logger.warning(f\"Error in loading {table_key}, pass\")\n",
    "        else:\n",
    "            if L1 != L2:\n",
    "                S1 = int(L1 ** 0.5)\n",
    "                S2 = int(L2 ** 0.5)\n",
    "                table_pretrained_resized = F.interpolate(\n",
    "                     table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n",
    "                     size=(S2, S2), mode='bicubic')\n",
    "                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n",
    "\n",
    "    # load state_dict\n",
    "    load_state_dict(model, state_dict, strict, logger)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  \n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrain_img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 depths=[2, 2, 6, 2],\n",
    "                 num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 out_indices=(0, 1, 2, 3),\n",
    "                 frozen_stages=-1,\n",
    "                 use_checkpoint=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.out_indices = out_indices\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]\n",
    "\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "        self.num_features = num_features\n",
    "#         self.norm = norm_layer(self.num_features)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in out_indices:\n",
    "            layer = norm_layer(num_features[i_layer])\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "        self._freeze_stages()\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1 and self.ape:\n",
    "            self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 2:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages - 1):\n",
    "                m = self.layers[i]\n",
    "                m.eval()\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        if isinstance(pretrained, str):\n",
    "            self.apply(_init_weights)\n",
    "            load_checkpoint(self, pretrained, strict=False, logger=None)\n",
    "        elif pretrained is None:\n",
    "            self.apply(_init_weights)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        if self.ape:\n",
    "            # interpolate the position embedding to the corresponding size\n",
    "            absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "        else:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "                norm_layer = getattr(self, f'norm{i}')\n",
    "                x_out = norm_layer(x_out)\n",
    "\n",
    "                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "                outs.append(out)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "        super(SwinTransformer, self).train(mode)\n",
    "        self._freeze_stages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = SwinTransformer(frozen_stages=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask\n",
      "\n",
      "missing keys in source state_dict: norm0.weight, norm0.bias, norm1.weight, norm1.bias, norm2.weight, norm2.bias, norm3.weight, norm3.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# backbone.init_weights('/home/ubuntu/MyFiles/swin_tiny_patch4_window7_224.pth')\n",
    "backbone.init_weights('e://毕业论文/swin_tiny_patch4_window7_224.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPM(nn.ModuleList):\n",
    "    def __init__(self, pool_sizes, in_channels=768, out_channels=256):\n",
    "        super(PPM, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        for pool_size in pool_sizes:\n",
    "            self.append(\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveMaxPool2d(pool_size),\n",
    "                    nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1),\n",
    "                )\n",
    "            )     \n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_puts = []\n",
    "        for ppm in self:\n",
    "            ppm_out = nn.functional.interpolate(ppm(x), size=(x.size(2), x.size(3)), mode='bilinear', align_corners=True)\n",
    "            out_puts.append(ppm_out)\n",
    "        return out_puts\n",
    " \n",
    "    \n",
    "class PPMHEAD(nn.Module):\n",
    "    def __init__(self, in_channels=768, out_channels=256, pool_sizes = [1, 2, 3, 6],num_classes=2):\n",
    "        super(PPMHEAD, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.psp_modules = PPM(self.pool_sizes, self.in_channels, self.out_channels)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels + len(self.pool_sizes)*self.out_channels, self.out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.psp_modules(x)\n",
    "        out.append(x)\n",
    "        out = torch.cat(out, 1)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    " \n",
    "class FPNHEAD(nn.Module):\n",
    "    def __init__(self, channels=768, out_channels=256):\n",
    "        super(FPNHEAD, self).__init__()\n",
    "        self.PPMHead = PPMHEAD(in_channels=channels, out_channels=out_channels)\n",
    "        \n",
    "        self.Conv_fuse1 = nn.Sequential(\n",
    "            nn.Conv2d(channels//2, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Conv_fuse1_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Conv_fuse2 = nn.Sequential(\n",
    "            nn.Conv2d(channels//4, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )    \n",
    "        self.Conv_fuse2_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.Conv_fuse3 = nn.Sequential(\n",
    "            nn.Conv2d(channels//8, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        self.Conv_fuse3_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        self.fuse_all = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*4, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_x1 = nn.Conv2d(out_channels, out_channels, 1)\n",
    " \n",
    "    def forward(self, input_fpn):\n",
    "        # b, 768,16,16\n",
    "        x1 = self.PPMHead(input_fpn[-1])\n",
    "         #b, 256,32,32\n",
    "        x = nn.functional.interpolate(x1, size=(x1.size(2)*2, x1.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = self.conv_x1(x) + self.Conv_fuse1(input_fpn[-2])  #b,256,32,32\n",
    "        x2 = self.Conv_fuse1_(x)\n",
    "        \n",
    "        x = nn.functional.interpolate(x2, size=(x2.size(2)*2, x2.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = x + self.Conv_fuse2(input_fpn[-3]) #b,256, 64, 64\n",
    "        x3 = self.Conv_fuse2_(x)  \n",
    " \n",
    "        x = nn.functional.interpolate(x3, size=(x3.size(2)*2, x3.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = x + self.Conv_fuse3(input_fpn[-4]) #b,256,128,128\n",
    "        x4 = self.Conv_fuse3_(x)\n",
    " \n",
    "        x1 = F.interpolate(x1, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    "        x2 = F.interpolate(x2, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    "        x3 = F.interpolate(x3, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    " \n",
    "        x = self.fuse_all(torch.cat([x1, x2, x3, x4], 1)) #b,256*4,128,128\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class UPerNet(nn.Module):\n",
    "    def __init__(self, num_classes, backbone):\n",
    "        super(UPerNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.in_channels = 768\n",
    "        self.channels = 256\n",
    "        self.decoder = FPNHEAD()\n",
    "        self.cls_seg = nn.Sequential(\n",
    "            nn.Conv2d(self.channels, self.num_classes, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        x = nn.functional.interpolate(x, size=(x.size(2)*4, x.size(3)*4),mode='bilinear', align_corners=True)\n",
    "        x = self.cls_seg(x)\n",
    "        return x\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_hist(a, b, n):\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    #   a是转化成一维数组的标签，形状(H×W,)；b是转化成一维数组的预测结果，形状(H×W,)\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    k = (a >= 0) & (a < n)\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    #   np.bincount计算了从0到n**2-1这n**2个数中每个数出现的次数，返回值形状(n, n)\n",
    "    #   返回中，写对角线上的为分类正确的像素点\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)  \n",
    "\n",
    "def per_class_iu(hist):\n",
    "    return np.diag(hist) / np.maximum((hist.sum(1) + hist.sum(0) - np.diag(hist)), 1) \n",
    "\n",
    "def per_class_PA_Recall(hist):\n",
    "    return np.diag(hist) / np.maximum(hist.sum(1), 1) \n",
    "\n",
    "def per_class_Precision(hist):\n",
    "    return np.diag(hist) / np.maximum(hist.sum(0), 1) \n",
    "\n",
    "def per_Accuracy(hist):\n",
    "    return np.sum(np.diag(hist)) / np.maximum(np.sum(hist), 1) \n",
    "\n",
    "def compute_mIoU(gt_dir, pred_dir, png_name_list, num_classes, name_classes=None):  \n",
    "    print('Num classes', num_classes)  \n",
    "    #-----------------------------------------#\n",
    "    #   创建一个全是0的矩阵，是一个混淆矩阵\n",
    "    #-----------------------------------------#\n",
    "    hist = np.zeros((num_classes, num_classes))  #2*2\n",
    "    \n",
    "    #------------------------------------------------#\n",
    "    #   获得验证集标签路径列表，方便直接读取\n",
    "    #   获得验证集图像分割结果路径列表，方便直接读取\n",
    "    #------------------------------------------------#\n",
    "    gt_imgs     = [os.path.join(gt_dir, x + \"_segmentation.png\") for x in png_name_list]  \n",
    "    pred_imgs   = [os.path.join(pred_dir, x + \".png\") for x in png_name_list]  \n",
    "\n",
    "    #------------------------------------------------#\n",
    "    #   读取每一个（图片-标签）对\n",
    "    #------------------------------------------------#\n",
    "    for ind in range(len(gt_imgs)): \n",
    "        #------------------------------------------------#\n",
    "        #   读取一张图像分割结果，转化成numpy数组\n",
    "        #------------------------------------------------#\n",
    "        pred = np.array(Image.open(pred_imgs[ind]))  #0为   1为背景\n",
    "        #------------------------------------------------#\n",
    "        #   读取一张对应的标签，转化成numpy数组\n",
    "        #------------------------------------------------#\n",
    "        png = np.array(Image.open(gt_imgs[ind])) #0为背景， 255为目标\n",
    "        label  = np.zeros_like(png)\n",
    "        label[png <= 127.5] = 1\n",
    "\n",
    "        # 如果图像分割结果与标签的大小不一样，这张图片就不计算\n",
    "        if len(label.flatten()) != len(pred.flatten()):  \n",
    "            print(\n",
    "                'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}'.format(\n",
    "                    len(label.flatten()), len(pred.flatten()), gt_imgs[ind],\n",
    "                    pred_imgs[ind]))\n",
    "            continue\n",
    "\n",
    "        #------------------------------------------------#\n",
    "        #   对一张图片计算21×21的hist矩阵，并累加\n",
    "        #------------------------------------------------#\n",
    "        hist += fast_hist(label.flatten(), pred.flatten(), num_classes)  \n",
    "        # 每计算10张就输出一下目前已计算的图片中所有类别平均的mIoU值\n",
    "        if name_classes is not None and ind > 0 and ind % 10 == 0: \n",
    "            print('{:d} / {:d}: mIou-{:0.2f}%; mPA-{:0.2f}%; Accuracy-{:0.2f}%'.format(\n",
    "                    ind, \n",
    "                    len(gt_imgs),\n",
    "                    100 * np.nanmean(per_class_iu(hist)),\n",
    "                    100 * np.nanmean(per_class_PA_Recall(hist)),\n",
    "                    100 * per_Accuracy(hist)\n",
    "                )\n",
    "            )\n",
    "    #------------------------------------------------#\n",
    "    #   计算所有验证集图片的逐类别mIoU值\n",
    "    #------------------------------------------------#\n",
    "    IoUs        = per_class_iu(hist)\n",
    "    PA_Recall   = per_class_PA_Recall(hist)\n",
    "    Precision   = per_class_Precision(hist)\n",
    "    #------------------------------------------------#\n",
    "    #   逐类别输出一下mIoU值\n",
    "    #------------------------------------------------#\n",
    "    if name_classes is not None:\n",
    "        for ind_class in range(num_classes):\n",
    "            print('===>' + name_classes[ind_class] + ':\\tIou-' + str(round(IoUs[ind_class] * 100, 2)) \\\n",
    "                + '; Recall (equal to the PA)-' + str(round(PA_Recall[ind_class] * 100, 2))+ '; Precision-' + str(round(Precision[ind_class] * 100, 2)))\n",
    "\n",
    "    #-----------------------------------------------------------------#\n",
    "    #   在所有验证集图像上求所有类别平均的mIoU值，计算时忽略NaN值\n",
    "    #-----------------------------------------------------------------#\n",
    "    print('===> mIoU: ' + str(round(np.nanmean(IoUs) * 100, 2)) + '; mPA: ' + str(round(np.nanmean(PA_Recall) * 100, 2)) + '; Accuracy: ' + str(round(per_Accuracy(hist) * 100, 2)))  \n",
    "    hist = np.array(hist)\n",
    "    return np.array(hist, np.int), IoUs, PA_Recall, Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory():\n",
    "    def __init__(self, log_dir, model, input_shape, val_loss_flag=True):\n",
    "        self.log_dir        = log_dir\n",
    "        self.val_loss_flag  = val_loss_flag\n",
    "\n",
    "        self.losses         = []\n",
    "        if self.val_loss_flag:\n",
    "            self.val_loss   = []\n",
    "        \n",
    "        os.makedirs(self.log_dir)\n",
    "        self.writer     = SummaryWriter(self.log_dir)\n",
    "        try:\n",
    "            dummy_input     = torch.randn(2, 3, input_shape[0], input_shape[1])\n",
    "            self.writer.add_graph(model, dummy_input)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def append_loss(self, epoch, loss, val_loss = None):\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        self.losses.append(loss)\n",
    "        if self.val_loss_flag:\n",
    "            self.val_loss.append(val_loss)\n",
    "        \n",
    "        with open(os.path.join(self.log_dir, \"epoch_loss.txt\"), 'a') as f:\n",
    "            f.write(str(loss))\n",
    "            f.write(\"\\n\")\n",
    "        if self.val_loss_flag:\n",
    "            with open(os.path.join(self.log_dir, \"epoch_val_loss.txt\"), 'a') as f:\n",
    "                f.write(str(val_loss))\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "        self.writer.add_scalar('loss', loss, epoch)\n",
    "        if self.val_loss_flag:\n",
    "            self.writer.add_scalar('val_loss', val_loss, epoch)\n",
    "            \n",
    "        self.loss_plot()\n",
    "\n",
    "    def loss_plot(self):\n",
    "        iters = range(len(self.losses))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses, 'red', linewidth = 2, label='train loss')\n",
    "        if self.val_loss_flag:\n",
    "            plt.plot(iters, self.val_loss, 'coral', linewidth = 2, label='val loss')\n",
    "            \n",
    "        try:\n",
    "            if len(self.losses) < 25:\n",
    "                num = 5\n",
    "            else:\n",
    "                num = 15\n",
    "            \n",
    "            plt.plot(iters, scipy.signal.savgol_filter(self.losses, num, 3), 'green', linestyle = '--', linewidth = 2, label='smooth train loss')\n",
    "            if self.val_loss_flag:\n",
    "                plt.plot(iters, scipy.signal.savgol_filter(self.val_loss, num, 3), '#8B4513', linestyle = '--', linewidth = 2, label='smooth val loss')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.savefig(os.path.join(self.log_dir, \"epoch_loss.png\"))\n",
    "\n",
    "        plt.cla()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "class EvalCallback():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(EvalCallback, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "        if self.eval_flag:\n",
    "            with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "                f.write(str(0))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def on_epoch_end(self, epoch, model_eval):\n",
    "        if epoch % self.period == 0 and self.eval_flag:\n",
    "            self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "            gt_dir      = os.path.join(self.dataset_path, \"Labels\")\n",
    "            pred_dir    = os.path.join(self.miou_out_path, 'detection-results')\n",
    "            if not os.path.exists(self.miou_out_path):\n",
    "                os.makedirs(self.miou_out_path)\n",
    "            if not os.path.exists(pred_dir):\n",
    "                os.makedirs(pred_dir)\n",
    "            print(\"Get miou.\")\n",
    "            for image_id in tqdm(self.image_ids):\n",
    "                #-------------------------------#\n",
    "                #   从文件中读取图像\n",
    "                #-------------------------------#\n",
    "                image_path  = os.path.join(self.dataset_path, \"Images/\"+image_id+\".jpg\")\n",
    "                image       = Image.open(image_path)\n",
    "                #------------------------------#\n",
    "                #   获得预测txt\n",
    "                #------------------------------#\n",
    "                image       = self.get_miou_png(image)\n",
    "                image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "                        \n",
    "            print(\"Calculate miou.\")\n",
    "            _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "            temp_miou = np.nanmean(IoUs) * 100\n",
    "\n",
    "            self.mious.append(temp_miou)\n",
    "            self.epoches.append(epoch)\n",
    "\n",
    "            with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "                f.write(str(temp_miou))\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.epoches, self.mious, 'red', linewidth = 2, label='train miou')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Miou')\n",
    "            plt.title('A Miou Curve')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "\n",
    "            plt.savefig(os.path.join(self.log_dir, \"epoch_miou.png\"))\n",
    "            plt.cla()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "            print(\"Get miou done.\")\n",
    "            shutil.rmtree(self.miou_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtColor(image):\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
    "        return image \n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image \n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def resize_image(image, size):\n",
    "    iw, ih  = image.size\n",
    "    w, h    = size\n",
    "\n",
    "    scale   = min(w/iw, h/ih)\n",
    "    nw      = int(iw*scale)\n",
    "    nh      = int(ih*scale)\n",
    "\n",
    "    image   = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, nw, nh\n",
    "\n",
    "class UnetDataset(Dataset):\n",
    "    def __init__(self, annotation_lines, input_shape, num_classes, train, dataset_path):\n",
    "        super(UnetDataset, self).__init__()\n",
    "        self.annotation_lines   = annotation_lines\n",
    "        self.length             = len(annotation_lines)\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.train              = train\n",
    "        self.dataset_path       = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annotation_line = self.annotation_lines[index]\n",
    "        name            = annotation_line.split()[0][:-4]\n",
    "\n",
    "        #-------------------------------#\n",
    "        #   从文件中读取图像\n",
    "        #-------------------------------#\n",
    "        jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "        png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "        #-------------------------------#\n",
    "        #   数据增强\n",
    "        #-------------------------------#\n",
    "#         jpg, png    = self.get_random_data(jpg, png, self.input_shape, random = self.train)\n",
    "        jpg, png    = self.get_random_data(jpg, png, self.input_shape, random = False)\n",
    "\n",
    "        jpg         = np.array(jpg)\n",
    "        jpg         = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2,0,1])\n",
    "        png         = np.array(png)\n",
    "        #-------------------------------------------------------#\n",
    "        #   这里的标签处理方式和普通voc的处理方式不同\n",
    "        #   将小于127.5的像素点设置为目标像素点。\n",
    "        #-------------------------------------------------------#\n",
    "        modify_png  = np.zeros_like(png)\n",
    "        modify_png[png <= 127.5] = 1\n",
    "        seg_labels  = modify_png\n",
    "        seg_labels  = np.eye(self.num_classes + 1)[seg_labels.reshape([-1])]\n",
    "        seg_labels  = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes + 1))\n",
    "#         seg_labels  = np.eye(self.num_classes)[seg_labels.reshape([-1])]\n",
    "#         seg_labels  = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes))\n",
    "\n",
    "        return jpg, modify_png, seg_labels\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.3, random=True):\n",
    "        image   = cvtColor(image)\n",
    "        label   = Image.fromarray(np.array(label))\n",
    "        #------------------------------#\n",
    "        #   获得图像的高宽与目标高宽\n",
    "        #------------------------------#\n",
    "        iw, ih  = image.size\n",
    "        h, w    = input_shape\n",
    "\n",
    "        if not random:\n",
    "            iw, ih  = image.size\n",
    "            scale   = min(w/iw, h/ih)\n",
    "            nw      = int(iw*scale)\n",
    "            nh      = int(ih*scale)\n",
    "\n",
    "            image       = image.resize((nw,nh), Image.BICUBIC)\n",
    "            new_image   = Image.new('RGB', [w, h], (128,128,128))\n",
    "            new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "            label       = label.resize((nw,nh), Image.NEAREST)\n",
    "            new_label   = Image.new('L', [w, h], (0))\n",
    "            new_label.paste(label, ((w-nw)//2, (h-nh)//2))\n",
    "            return new_image, new_label\n",
    "\n",
    "        #------------------------------------------#\n",
    "        #   对图像进行缩放并且进行长和宽的扭曲\n",
    "        #------------------------------------------#\n",
    "        new_ar = iw/ih * self.rand(1-jitter,1+jitter) / self.rand(1-jitter,1+jitter)\n",
    "        scale = self.rand(0.25, 2)\n",
    "        if new_ar < 1:\n",
    "            nh = int(scale*h)\n",
    "            nw = int(nh*new_ar)\n",
    "        else:\n",
    "            nw = int(scale*w)\n",
    "            nh = int(nw/new_ar)\n",
    "        image = image.resize((nw,nh), Image.BICUBIC)\n",
    "        label = label.resize((nw,nh), Image.NEAREST)\n",
    "        \n",
    "        #------------------------------------------#\n",
    "        #   翻转图像\n",
    "        #------------------------------------------#\n",
    "        flip = self.rand()<.5\n",
    "        if flip: \n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        #------------------------------------------#\n",
    "        #   将图像多余的部分加上灰条\n",
    "        #------------------------------------------#\n",
    "        dx = int(self.rand(0, w-nw))\n",
    "        dy = int(self.rand(0, h-nh))\n",
    "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "        new_label = Image.new('L', (w,h), (0))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        new_label.paste(label, (dx, dy))\n",
    "        image = new_image\n",
    "        label = new_label\n",
    "        image = np.array(image)\n",
    "        image_data      = np.array(image, np.uint8)\n",
    "        #---------------------------------#\n",
    "        #   对图像进行色域变换\n",
    "        #   计算色域变换的参数\n",
    "        #---------------------------------#\n",
    "        r               = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
    "        #---------------------------------#\n",
    "        #   将图像转到HSV上\n",
    "        #---------------------------------#\n",
    "        hue, sat, val   = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
    "        dtype           = image_data.dtype\n",
    "        #---------------------------------#\n",
    "        #   应用变换\n",
    "        #---------------------------------#\n",
    "        x       = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
    "        \n",
    "        return image_data, label\n",
    "\n",
    "# DataLoader中collate_fn使用\n",
    "def unet_dataset_collate(batch):\n",
    "    images      = []\n",
    "    pngs        = []\n",
    "    seg_labels  = []\n",
    "    for img, png, labels in batch:\n",
    "        images.append(img)\n",
    "        pngs.append(png)\n",
    "        seg_labels.append(labels)\n",
    "    images      = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
    "    pngs        = torch.from_numpy(np.array(pngs)).long() #batchsize, 512,512,    1表示背景\n",
    "    seg_labels  = torch.from_numpy(np.array(seg_labels)).type(torch.FloatTensor)#batchsize, 512,512, num_classes+1\n",
    "    return images, pngs, seg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda = True\n",
    "Cuda = False\n",
    "distributed     = False\n",
    "sync_bn         = False\n",
    "fp16            = False\n",
    "num_classes = 2\n",
    "pretrained  = True\n",
    "model_path  = \"\"\n",
    "input_shape = [224, 224]\n",
    "Init_Epoch          = 0\n",
    "Freeze_Epoch        = 25\n",
    "Freeze_batch_size   = 2\n",
    "UnFreeze_Epoch      = 50\n",
    "Unfreeze_batch_size = 2\n",
    "Freeze_Train        = True\n",
    "Init_lr             = 1e-4\n",
    "Min_lr              = Init_lr * 0.01\n",
    "optimizer_type      = \"adam\"\n",
    "momentum            = 0.9\n",
    "weight_decay        = 0\n",
    "lr_decay_type       = 'step'\n",
    "save_period         = 5\n",
    "save_dir            = 'logs'\n",
    "eval_flag           = True\n",
    "eval_period         = 5\n",
    "# VOCdevkit_path  = '/home/ubuntu/MyFiles/Skin_Datasets/train'\n",
    "VOCdevkit_path  = 'e://毕业论文/Skin_Datasets/train'\n",
    "dice_loss       = True\n",
    "focal_loss      = False\n",
    "cls_weights     = np.ones([num_classes], np.float32)\n",
    "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank      = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_lines_3.txt','r')as f:\n",
    "    total_lines = f.readlines()\n",
    "total_lines = [line.strip() for line in total_lines]\n",
    "# train_lines = total_lines[:1998]\n",
    "# val_lines = total_lines[1998:1998+348]\n",
    "train_lines = total_lines[:1998]\n",
    "val_lines = total_lines[1998:1998+348]\n",
    "num_train   = len(train_lines)\n",
    "num_val     = len(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPerNet(num_classes=num_classes,backbone=backbone).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ca123ed7f7a1>:317: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if W % self.patch_size[1] != 0:\n",
      "<ipython-input-4-ca123ed7f7a1>:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if H % self.patch_size[0] != 0:\n",
      "<ipython-input-4-ca123ed7f7a1>:263: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
      "<ipython-input-4-ca123ed7f7a1>:264: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "<ipython-input-4-ca123ed7f7a1>:133: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, \"input feature has wrong size\"\n",
      "<ipython-input-4-ca123ed7f7a1>:31: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
      "<ipython-input-4-ca123ed7f7a1>:171: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_r > 0 or pad_b > 0:\n",
      "<ipython-input-4-ca123ed7f7a1>:194: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, \"input feature has wrong size\"\n",
      "<ipython-input-4-ca123ed7f7a1>:199: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
      "<ipython-input-4-ca123ed7f7a1>:200: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_input:\n"
     ]
    }
   ],
   "source": [
    "time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train     = model.train()\n",
    "if Cuda:\n",
    "    model_train = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    model_train = model_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio = 0.05, warmup_lr_ratio = 0.1, no_aug_iter_ratio = 0.05, step_num = 10):\n",
    "    def yolox_warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters):\n",
    "        if iters <= warmup_total_iters:\n",
    "            # lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start\n",
    "            lr = (lr - warmup_lr_start) * pow(iters / float(warmup_total_iters), 2) + warmup_lr_start\n",
    "        elif iters >= total_iters - no_aug_iter:\n",
    "            lr = min_lr\n",
    "        else:\n",
    "            lr = min_lr + 0.5 * (lr - min_lr) * (\n",
    "                1.0 + math.cos(math.pi* (iters - warmup_total_iters) / (total_iters - warmup_total_iters - no_aug_iter))\n",
    "            )\n",
    "        return lr\n",
    "\n",
    "    def step_lr(lr, decay_rate, step_size, iters):\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must above 1.\")\n",
    "        n       = iters // step_size\n",
    "        out_lr  = lr * decay_rate ** n\n",
    "        return out_lr\n",
    "\n",
    "    if lr_decay_type == \"cos\":\n",
    "        warmup_total_iters  = min(max(warmup_iters_ratio * total_iters, 1), 3)\n",
    "        warmup_lr_start     = max(warmup_lr_ratio * lr, 1e-6)\n",
    "        no_aug_iter         = min(max(no_aug_iter_ratio * total_iters, 1), 15)\n",
    "        func = partial(yolox_warm_cos_lr ,lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter)\n",
    "    else:\n",
    "        decay_rate  = (min_lr / lr) ** (1 / (step_num - 1))\n",
    "        step_size   = total_iters / step_num\n",
    "        func = partial(step_lr, lr, decay_rate, step_size)\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnFreeze_flag = False\n",
    "# if Freeze_Train:\n",
    "#     model.freeze_backbone()\n",
    "batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
    "epoch_step      = num_train // batch_size\n",
    "epoch_step_val  = num_val // batch_size\n",
    "nbs = 16\n",
    "lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "optimizer = {\n",
    "    'adam'  : optim.Adam(model.parameters(), Init_lr_fit, betas = (momentum, 0.999), weight_decay = weight_decay),\n",
    "    'sgd'   : optim.SGD(model.parameters(), Init_lr_fit, momentum = momentum, nesterov=True, weight_decay = weight_decay)\n",
    "}[optimizer_type]\n",
    "lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "train_dataset   = UnetDataset(train_lines, input_shape, num_classes, True, VOCdevkit_path)\n",
    "val_dataset     = UnetDataset(val_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "train_sampler   = None\n",
    "val_sampler     = None\n",
    "shuffle         = True\n",
    "# gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=True,\n",
    "#                             drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "# gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                             drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=False,\n",
    "                            drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=False, \n",
    "                            drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "eval_callback   = EvalCallback(model, input_shape, num_classes, val_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_Loss(inputs, target, cls_weights, num_classes=2):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    temp_target = target.view(-1)  #1为背景\n",
    "\n",
    "    CE_loss  = nn.CrossEntropyLoss(weight=cls_weights, ignore_index=num_classes)(temp_inputs, temp_target)\n",
    "    return CE_loss\n",
    "\n",
    "def Dice_loss(inputs, target, beta=1, smooth = 1e-5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #--------------------------------------------#\n",
    "    #   计算dice loss\n",
    "    #--------------------------------------------#\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    dice_loss = 1 - torch.mean(score)\n",
    "    return dice_loss\n",
    "\n",
    "def f_score(inputs, target, beta=1, smooth = 1e-5, threhold = 0.5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #--------------------------------------------#\n",
    "    #   计算dice系数\n",
    "    #--------------------------------------------#\n",
    "    temp_inputs = torch.gt(temp_inputs, threhold).float()\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    score = torch.mean(score)\n",
    "    return score\n",
    "\n",
    "def fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank=0):\n",
    "    total_loss      = 0\n",
    "    total_f_score   = 0\n",
    "\n",
    "    val_loss        = 0\n",
    "    val_f_score     = 0\n",
    "\n",
    "    if local_rank == 0:\n",
    "        print('Start Train')\n",
    "        pbar = tqdm(total=epoch_step,desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
    "    model_train.train()\n",
    "    for iteration, batch in enumerate(gen):\n",
    "        if iteration >= epoch_step: \n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs    = imgs.cuda(local_rank)\n",
    "                pngs    = pngs.cuda(local_rank)\n",
    "                labels  = labels.cuda(local_rank)\n",
    "                weights = weights.cuda(local_rank)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if not fp16:\n",
    "            #----------------------#\n",
    "            #   前向传播\n",
    "            #----------------------#\n",
    "            outputs = model_train(imgs)\n",
    "            #----------------------#\n",
    "            #   损失计算\n",
    "            #----------------------#\n",
    "#             if focal_loss:\n",
    "#                 loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "#             else:\n",
    "            loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "            if dice_loss:\n",
    "                main_dice = Dice_loss(outputs, labels)\n",
    "                loss      = loss + main_dice\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #-------------------------------#\n",
    "                #   计算f_score\n",
    "                #-------------------------------#\n",
    "                _f_score = f_score(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            from torch.cuda.amp import autocast\n",
    "            with autocast():\n",
    "                #----------------------#\n",
    "                #   前向传播\n",
    "                #----------------------#\n",
    "                outputs = model_train(imgs)\n",
    "                #----------------------#\n",
    "                #   损失计算\n",
    "                #----------------------#\n",
    "                if focal_loss:\n",
    "                    loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "                else:\n",
    "                    loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "                if dice_loss:\n",
    "                    main_dice = Dice_loss(outputs, labels)\n",
    "                    loss      = loss + main_dice\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    #-------------------------------#\n",
    "                    #   计算f_score\n",
    "                    #-------------------------------#\n",
    "                    _f_score = f_score(outputs, labels)\n",
    "\n",
    "            #----------------------#\n",
    "            #   反向传播\n",
    "            #----------------------#\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_loss      += loss.item()\n",
    "        total_f_score   += _f_score.item()\n",
    "        \n",
    "        if local_rank == 0:\n",
    "            pbar.set_postfix(**{'total_loss': total_loss / (iteration + 1), \n",
    "                                'f_score'   : total_f_score / (iteration + 1),\n",
    "                                'lr'        : get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        pbar.close()\n",
    "        print('Finish Train')\n",
    "        print('Start Validation')\n",
    "        pbar = tqdm(total=epoch_step_val, desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
    "\n",
    "    model_train.eval()\n",
    "    for iteration, batch in enumerate(gen_val):\n",
    "        if iteration >= epoch_step_val:\n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs    = imgs.cuda(local_rank)\n",
    "                pngs    = pngs.cuda(local_rank)\n",
    "                labels  = labels.cuda(local_rank)\n",
    "                weights = weights.cuda(local_rank)\n",
    "\n",
    "            #----------------------#\n",
    "            #   前向传播\n",
    "            #----------------------#\n",
    "            outputs = model_train(imgs)\n",
    "            #----------------------#\n",
    "            #   损失计算\n",
    "            #----------------------#\n",
    "#             if focal_loss:\n",
    "#                 loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "#             else:\n",
    "            loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "            if dice_loss:\n",
    "                main_dice = Dice_loss(outputs, labels)\n",
    "                loss  = loss + main_dice\n",
    "            #-------------------------------#\n",
    "            #   计算f_score\n",
    "            #-------------------------------#\n",
    "            _f_score    = f_score(outputs, labels)\n",
    "\n",
    "            val_loss    += loss.item()\n",
    "            val_f_score += _f_score.item()\n",
    "            \n",
    "        if local_rank == 0:\n",
    "            pbar.set_postfix(**{'val_loss'  : val_loss / (iteration + 1),\n",
    "                                'f_score'   : val_f_score / (iteration + 1),\n",
    "                                'lr'        : get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "            \n",
    "    if local_rank == 0:\n",
    "        pbar.close()\n",
    "        print('Finish Validation')\n",
    "        loss_history.append_loss(epoch + 1, total_loss/ epoch_step, val_loss/ epoch_step_val)\n",
    "        eval_callback.on_epoch_end(epoch + 1, model_train)\n",
    "        print('Epoch:'+ str(epoch+1) + '/' + str(Epoch))\n",
    "        print('Total Loss: %.3f || Val Loss: %.3f ' % (total_loss / epoch_step, val_loss / epoch_step_val))\n",
    "        \n",
    "        #-----------------------------------------------#\n",
    "        #   保存权值\n",
    "        #-----------------------------------------------#\n",
    "        if (epoch + 1) % save_period == 0 or epoch + 1 == Epoch:\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'ep%03d-loss%.3f-val_loss%.3f.pth'%((epoch + 1), total_loss / epoch_step, val_loss / epoch_step_val)))\n",
    "\n",
    "#         if len(loss_history.val_loss) <= 1 or (val_loss / epoch_step_val) <= min(loss_history.val_loss):\n",
    "#             print('Save best model to best_epoch_weights.pth')\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_epoch_weights.pth\"))\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"last_epoch_weights.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 999/999 [14:51<00:00,  1.12it/s, f_score=0.893, lr=0.0001, total_loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 174/174 [02:31<00:00,  1.15it/s, f_score=0.913, lr=0.0001, val_loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:1/50\n",
      "Total Loss: 0.278 || Val Loss: 0.214 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 999/999 [14:06<00:00,  1.18it/s, f_score=0.922, lr=0.0001, total_loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 174/174 [02:14<00:00,  1.29it/s, f_score=0.924, lr=0.0001, val_loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:2/50\n",
      "Total Loss: 0.199 || Val Loss: 0.195 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 999/999 [13:59<00:00,  1.19it/s, f_score=0.933, lr=0.0001, total_loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 174/174 [02:16<00:00,  1.28it/s, f_score=0.93, lr=0.0001, val_loss=0.182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:3/50\n",
      "Total Loss: 0.171 || Val Loss: 0.182 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 999/999 [15:40<00:00,  1.06it/s, f_score=0.937, lr=0.0001, total_loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 174/174 [02:33<00:00,  1.13it/s, f_score=0.93, lr=0.0001, val_loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:4/50\n",
      "Total Loss: 0.161 || Val Loss: 0.183 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 999/999 [15:00<00:00,  1.11it/s, f_score=0.939, lr=0.0001, total_loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.928, lr=0.0001, val_loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:39<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 84.88; mPA: 90.08; Accuracy: 94.34\n",
      "Get miou done.\n",
      "Epoch:5/50\n",
      "Total Loss: 0.153 || Val Loss: 0.190 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 999/999 [17:01<00:00,  1.02s/it, f_score=0.945, lr=5.99e-5, total_loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 174/174 [02:26<00:00,  1.19it/s, f_score=0.934, lr=5.99e-5, val_loss=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:6/50\n",
      "Total Loss: 0.135 || Val Loss: 0.176 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 999/999 [15:09<00:00,  1.10it/s, f_score=0.95, lr=5.99e-5, total_loss=0.123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 174/174 [02:36<00:00,  1.11it/s, f_score=0.931, lr=5.99e-5, val_loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:7/50\n",
      "Total Loss: 0.123 || Val Loss: 0.178 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 999/999 [15:49<00:00,  1.05it/s, f_score=0.952, lr=5.99e-5, total_loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 174/174 [02:39<00:00,  1.09it/s, f_score=0.93, lr=5.99e-5, val_loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:8/50\n",
      "Total Loss: 0.117 || Val Loss: 0.179 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 999/999 [15:17<00:00,  1.09it/s, f_score=0.954, lr=5.99e-5, total_loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 174/174 [02:32<00:00,  1.14it/s, f_score=0.936, lr=5.99e-5, val_loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:9/50\n",
      "Total Loss: 0.111 || Val Loss: 0.173 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 999/999 [15:03<00:00,  1.11it/s, f_score=0.956, lr=5.99e-5, total_loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.931, lr=5.99e-5, val_loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [03:48<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.47; mPA: 91.52; Accuracy: 94.44\n",
      "Get miou done.\n",
      "Epoch:10/50\n",
      "Total Loss: 0.105 || Val Loss: 0.193 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 999/999 [15:27<00:00,  1.08it/s, f_score=0.961, lr=3.59e-5, total_loss=0.0933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 174/174 [02:13<00:00,  1.30it/s, f_score=0.933, lr=3.59e-5, val_loss=0.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:11/50\n",
      "Total Loss: 0.093 || Val Loss: 0.194 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 999/999 [15:15<00:00,  1.09it/s, f_score=0.963, lr=3.59e-5, total_loss=0.0905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.934, lr=3.59e-5, val_loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:12/50\n",
      "Total Loss: 0.090 || Val Loss: 0.191 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 999/999 [14:54<00:00,  1.12it/s, f_score=0.964, lr=3.59e-5, total_loss=0.0852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 174/174 [02:36<00:00,  1.11it/s, f_score=0.933, lr=3.59e-5, val_loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:13/50\n",
      "Total Loss: 0.085 || Val Loss: 0.200 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 999/999 [15:48<00:00,  1.05it/s, f_score=0.966, lr=3.59e-5, total_loss=0.0791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 174/174 [02:44<00:00,  1.06it/s, f_score=0.933, lr=3.59e-5, val_loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:14/50\n",
      "Total Loss: 0.079 || Val Loss: 0.204 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 999/999 [15:10<00:00,  1.10it/s, f_score=0.968, lr=3.59e-5, total_loss=0.0762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.935, lr=3.59e-5, val_loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [03:59<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 86.48; mPA: 93.06; Accuracy: 94.76\n",
      "Get miou done.\n",
      "Epoch:15/50\n",
      "Total Loss: 0.076 || Val Loss: 0.200 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 999/999 [15:27<00:00,  1.08it/s, f_score=0.969, lr=2.15e-5, total_loss=0.0729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 174/174 [02:21<00:00,  1.23it/s, f_score=0.932, lr=2.15e-5, val_loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:16/50\n",
      "Total Loss: 0.073 || Val Loss: 0.205 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 999/999 [15:11<00:00,  1.10it/s, f_score=0.971, lr=2.15e-5, total_loss=0.0687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 174/174 [02:35<00:00,  1.12it/s, f_score=0.934, lr=2.15e-5, val_loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:17/50\n",
      "Total Loss: 0.069 || Val Loss: 0.216 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 999/999 [15:59<00:00,  1.04it/s, f_score=0.972, lr=2.15e-5, total_loss=0.065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 174/174 [02:51<00:00,  1.01it/s, f_score=0.934, lr=2.15e-5, val_loss=0.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:18/50\n",
      "Total Loss: 0.065 || Val Loss: 0.210 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 999/999 [17:08<00:00,  1.03s/it, f_score=0.973, lr=2.15e-5, total_loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 174/174 [02:50<00:00,  1.02it/s, f_score=0.932, lr=2.15e-5, val_loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:19/50\n",
      "Total Loss: 0.064 || Val Loss: 0.216 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 999/999 [15:49<00:00,  1.05it/s, f_score=0.973, lr=2.15e-5, total_loss=0.0635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 174/174 [02:44<00:00,  1.06it/s, f_score=0.927, lr=2.15e-5, val_loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.4; mPA: 91.34; Accuracy: 94.43\n",
      "Get miou done.\n",
      "Epoch:20/50\n",
      "Total Loss: 0.064 || Val Loss: 0.222 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 999/999 [15:36<00:00,  1.07it/s, f_score=0.976, lr=1.29e-5, total_loss=0.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 174/174 [02:16<00:00,  1.27it/s, f_score=0.934, lr=1.29e-5, val_loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:21/50\n",
      "Total Loss: 0.058 || Val Loss: 0.228 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 999/999 [14:50<00:00,  1.12it/s, f_score=0.976, lr=1.29e-5, total_loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.932, lr=1.29e-5, val_loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:22/50\n",
      "Total Loss: 0.057 || Val Loss: 0.232 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 999/999 [15:51<00:00,  1.05it/s, f_score=0.977, lr=1.29e-5, total_loss=0.0546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 174/174 [02:33<00:00,  1.13it/s, f_score=0.933, lr=1.29e-5, val_loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:23/50\n",
      "Total Loss: 0.055 || Val Loss: 0.235 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 999/999 [17:11<00:00,  1.03s/it, f_score=0.976, lr=1.29e-5, total_loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 174/174 [02:45<00:00,  1.05it/s, f_score=0.931, lr=1.29e-5, val_loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:24/50\n",
      "Total Loss: 0.057 || Val Loss: 0.228 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 999/999 [15:41<00:00,  1.06it/s, f_score=0.977, lr=1.29e-5, total_loss=0.0538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 174/174 [02:31<00:00,  1.15it/s, f_score=0.933, lr=1.29e-5, val_loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:05<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.79; mPA: 91.28; Accuracy: 94.63\n",
      "Get miou done.\n",
      "Epoch:25/50\n",
      "Total Loss: 0.054 || Val Loss: 0.224 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 999/999 [15:10<00:00,  1.10it/s, f_score=0.978, lr=7.74e-6, total_loss=0.051] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 174/174 [02:23<00:00,  1.21it/s, f_score=0.93, lr=7.74e-6, val_loss=0.242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:26/50\n",
      "Total Loss: 0.051 || Val Loss: 0.242 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 999/999 [15:01<00:00,  1.11it/s, f_score=0.979, lr=7.74e-6, total_loss=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 174/174 [02:31<00:00,  1.15it/s, f_score=0.933, lr=7.74e-6, val_loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:27/50\n",
      "Total Loss: 0.050 || Val Loss: 0.237 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 999/999 [16:14<00:00,  1.02it/s, f_score=0.979, lr=7.74e-6, total_loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 174/174 [02:37<00:00,  1.11it/s, f_score=0.93, lr=7.74e-6, val_loss=0.243] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:28/50\n",
      "Total Loss: 0.049 || Val Loss: 0.243 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 999/999 [15:36<00:00,  1.07it/s, f_score=0.979, lr=7.74e-6, total_loss=0.0491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 174/174 [02:30<00:00,  1.16it/s, f_score=0.93, lr=7.74e-6, val_loss=0.242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:29/50\n",
      "Total Loss: 0.049 || Val Loss: 0.242 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 999/999 [17:27<00:00,  1.05s/it, f_score=0.979, lr=7.74e-6, total_loss=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 174/174 [02:33<00:00,  1.13it/s, f_score=0.932, lr=7.74e-6, val_loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [03:56<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.22; mPA: 90.62; Accuracy: 94.43\n",
      "Get miou done.\n",
      "Epoch:30/50\n",
      "Total Loss: 0.048 || Val Loss: 0.253 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 999/999 [15:00<00:00,  1.11it/s, f_score=0.98, lr=4.64e-6, total_loss=0.0474] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 174/174 [02:22<00:00,  1.22it/s, f_score=0.932, lr=4.64e-6, val_loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:31/50\n",
      "Total Loss: 0.047 || Val Loss: 0.245 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 999/999 [15:10<00:00,  1.10it/s, f_score=0.98, lr=4.64e-6, total_loss=0.0466] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 174/174 [02:35<00:00,  1.12it/s, f_score=0.931, lr=4.64e-6, val_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:32/50\n",
      "Total Loss: 0.047 || Val Loss: 0.258 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 999/999 [14:59<00:00,  1.11it/s, f_score=0.981, lr=4.64e-6, total_loss=0.0459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 174/174 [02:35<00:00,  1.12it/s, f_score=0.931, lr=4.64e-6, val_loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:33/50\n",
      "Total Loss: 0.046 || Val Loss: 0.253 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 999/999 [15:14<00:00,  1.09it/s, f_score=0.98, lr=4.64e-6, total_loss=0.0464] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 174/174 [02:36<00:00,  1.11it/s, f_score=0.931, lr=4.64e-6, val_loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:34/50\n",
      "Total Loss: 0.046 || Val Loss: 0.256 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 999/999 [14:48<00:00,  1.12it/s, f_score=0.98, lr=4.64e-6, total_loss=0.0471] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 174/174 [02:43<00:00,  1.06it/s, f_score=0.933, lr=4.64e-6, val_loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:09<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.44; mPA: 91.11; Accuracy: 94.48\n",
      "Get miou done.\n",
      "Epoch:35/50\n",
      "Total Loss: 0.047 || Val Loss: 0.262 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 999/999 [15:26<00:00,  1.08it/s, f_score=0.981, lr=2.78e-6, total_loss=0.0453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 174/174 [02:12<00:00,  1.31it/s, f_score=0.931, lr=2.78e-6, val_loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:36/50\n",
      "Total Loss: 0.045 || Val Loss: 0.263 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 999/999 [15:30<00:00,  1.07it/s, f_score=0.981, lr=2.78e-6, total_loss=0.045] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 174/174 [02:55<00:00,  1.01s/it, f_score=0.931, lr=2.78e-6, val_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:37/50\n",
      "Total Loss: 0.045 || Val Loss: 0.258 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 999/999 [15:05<00:00,  1.10it/s, f_score=0.981, lr=2.78e-6, total_loss=0.0452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 174/174 [02:36<00:00,  1.11it/s, f_score=0.931, lr=2.78e-6, val_loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:38/50\n",
      "Total Loss: 0.045 || Val Loss: 0.252 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 999/999 [15:36<00:00,  1.07it/s, f_score=0.981, lr=2.78e-6, total_loss=0.0445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 174/174 [02:28<00:00,  1.17it/s, f_score=0.933, lr=2.78e-6, val_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:39/50\n",
      "Total Loss: 0.044 || Val Loss: 0.258 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 999/999 [17:22<00:00,  1.04s/it, f_score=0.981, lr=2.78e-6, total_loss=0.0444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 174/174 [02:31<00:00,  1.15it/s, f_score=0.932, lr=2.78e-6, val_loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [03:53<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.25; mPA: 90.73; Accuracy: 94.44\n",
      "Get miou done.\n",
      "Epoch:40/50\n",
      "Total Loss: 0.044 || Val Loss: 0.259 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 999/999 [16:14<00:00,  1.02it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 174/174 [02:16<00:00,  1.27it/s, f_score=0.931, lr=1.67e-6, val_loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:41/50\n",
      "Total Loss: 0.044 || Val Loss: 0.259 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 999/999 [15:23<00:00,  1.08it/s, f_score=0.982, lr=1.67e-6, total_loss=0.0429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 174/174 [02:33<00:00,  1.13it/s, f_score=0.932, lr=1.67e-6, val_loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:42/50\n",
      "Total Loss: 0.043 || Val Loss: 0.250 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 999/999 [14:53<00:00,  1.12it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 174/174 [02:32<00:00,  1.14it/s, f_score=0.932, lr=1.67e-6, val_loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:43/50\n",
      "Total Loss: 0.044 || Val Loss: 0.254 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 999/999 [16:04<00:00,  1.04it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 174/174 [02:56<00:00,  1.01s/it, f_score=0.932, lr=1.67e-6, val_loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:44/50\n",
      "Total Loss: 0.044 || Val Loss: 0.262 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 999/999 [15:14<00:00,  1.09it/s, f_score=0.982, lr=1.67e-6, total_loss=0.0435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 174/174 [02:38<00:00,  1.10it/s, f_score=0.935, lr=1.67e-6, val_loss=0.25] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:33<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.3; mPA: 90.79; Accuracy: 94.45\n",
      "Get miou done.\n",
      "Epoch:45/50\n",
      "Total Loss: 0.043 || Val Loss: 0.250 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 999/999 [16:37<00:00,  1.00it/s, f_score=0.982, lr=1e-6, total_loss=0.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 174/174 [02:43<00:00,  1.06it/s, f_score=0.929, lr=1e-6, val_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:46/50\n",
      "Total Loss: 0.043 || Val Loss: 0.258 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 999/999 [15:43<00:00,  1.06it/s, f_score=0.982, lr=1e-6, total_loss=0.043] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 174/174 [02:39<00:00,  1.09it/s, f_score=0.928, lr=1e-6, val_loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:47/50\n",
      "Total Loss: 0.043 || Val Loss: 0.254 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 999/999 [18:21<00:00,  1.10s/it, f_score=0.982, lr=1e-6, total_loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 174/174 [03:00<00:00,  1.04s/it, f_score=0.933, lr=1e-6, val_loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:48/50\n",
      "Total Loss: 0.043 || Val Loss: 0.259 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 999/999 [16:34<00:00,  1.00it/s, f_score=0.982, lr=1e-6, total_loss=0.043] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 174/174 [02:32<00:00,  1.14it/s, f_score=0.933, lr=1e-6, val_loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:49/50\n",
      "Total Loss: 0.043 || Val Loss: 0.261 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 999/999 [15:26<00:00,  1.08it/s, f_score=0.982, lr=1e-6, total_loss=0.0424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 174/174 [02:35<00:00,  1.12it/s, f_score=0.931, lr=1e-6, val_loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:22<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 85.16; mPA: 90.77; Accuracy: 94.38\n",
      "Get miou done.\n",
      "Epoch:50/50\n",
      "Total Loss: 0.042 || Val Loss: 0.264 \n"
     ]
    }
   ],
   "source": [
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
    "    #---------------------------------------#\n",
    "    #   如果模型有冻结学习部分\n",
    "    #   则解冻，并设置参数\n",
    "    #---------------------------------------#\n",
    "    if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
    "        batch_size = Unfreeze_batch_size\n",
    "\n",
    "        #-------------------------------------------------------------------#\n",
    "        #   判断当前batch_size，自适应调整学习率\n",
    "        #-------------------------------------------------------------------#\n",
    "        lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "        lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "        #---------------------------------------#\n",
    "        #   获得学习率下降的公式\n",
    "        #---------------------------------------#\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "\n",
    "        model.unfreeze_backbone()\n",
    "\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val // batch_size\n",
    "\n",
    "        if epoch_step == 0:\n",
    "            raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n",
    "\n",
    "\n",
    "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=True,\n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "\n",
    "        UnFreeze_flag = True\n",
    "\n",
    "    set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "    fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, \n",
    "                epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank)\n",
    "\n",
    "if local_rank == 0:\n",
    "    loss_history.writer.close()\n",
    "#     fit_one_epoch_no_val(model_train, model, loss_history, optimizer, epoch, epoch_step, gen, UnFreeze_Epoch, Cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [04:08<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 86.62; mPA: 91.48; Accuracy: 94.85\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [01:08<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test_lines = total_lines[1998+348:]\n",
    "# test_dataset = UnetDataset(test_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "# test_sampler = None\n",
    "# gen_test = DataLoader(test_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                                     drop_last = True, collate_fn = unet_dataset_collate, sampler=test_sampler)\n",
    "class GetTestResult():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(GetTestResult, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "#         if self.eval_flag:\n",
    "#             with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "#                 f.write(str(0))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def get_result(self, model_eval):\n",
    "        self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "        gt_dir      = os.path.join(self.dataset_path, \"Labels\")\n",
    "        pred_dir    = '/home/ubuntu/user_space/detection-results'\n",
    "        if not os.path.exists(self.miou_out_path):\n",
    "            os.makedirs(self.miou_out_path)\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "        print(\"Get miou.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            #-------------------------------#\n",
    "            #   从文件中读取图像\n",
    "            #-------------------------------#\n",
    "            image_path  = os.path.join(self.dataset_path, \"Images/\"+image_id+\".jpg\")\n",
    "            image       = Image.open(image_path)\n",
    "            #------------------------------#\n",
    "            #   获得预测txt\n",
    "            #------------------------------#\n",
    "            image       = self.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "\n",
    "        print(\"Calculate miou.\")\n",
    "        _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "        temp_miou = np.nanmean(IoUs) * 100\n",
    "        print(\"Get predict image.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            image_path = os.path.join(pred_dir, image_id + \".png\")\n",
    "            image = Image.open(image_path)\n",
    "            result = Image.fromarray(np.uint8((1-np.array(image))*255))\n",
    "            result.save(image_path)\n",
    "    \n",
    "GetResult = GetTestResult(model, input_shape, num_classes, test_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)\n",
    "GetResult.get_result(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('d://swin-tran-total-line3/ep050-loss0.042-val_loss0.264.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:08<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-39602163b541>:94: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(hist, np.int), IoUs, PA_Recall, Precision\n",
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> mIoU: 83.06; mPA: 90.24; Accuracy: 92.24\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:42<00:00,  9.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_lines = total_lines[1998+348:]\n",
    "# test_dataset = UnetDataset(test_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "# test_sampler = None\n",
    "# gen_test = DataLoader(test_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                                     drop_last = True, collate_fn = unet_dataset_collate, sampler=test_sampler)\n",
    "data_path = 'D://360Downloads//ISIC2018_Task1-2_Test_Input'\n",
    "test_lines = os.listdir(data_path)[1:-1]\n",
    "class GetTestResult():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(GetTestResult, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "#         if self.eval_flag:\n",
    "#             with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "#                 f.write(str(0))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def get_result(self, model_eval):\n",
    "        self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "        gt_dir      = 'e://毕业论文/ISIC2018_Task1_Test_GroundTruth'\n",
    "#         pred_dir    = '/home/ubuntu/user_space/detection-results'\n",
    "        pred_dir = 'd://swin-tran-total-line3/test-results50'\n",
    "        if not os.path.exists(self.miou_out_path):\n",
    "            os.makedirs(self.miou_out_path)\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "        print(\"Get miou.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            #-------------------------------#\n",
    "            #   从文件中读取图像\n",
    "            #-------------------------------#\n",
    "            image_path  = os.path.join(self.dataset_path, image_id+'.jpg')\n",
    "            image       = Image.open(image_path)\n",
    "            #------------------------------#\n",
    "            #   获得预测txt\n",
    "            #------------------------------#\n",
    "            image       = self.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "\n",
    "        print(\"Calculate miou.\")\n",
    "        _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "        temp_miou = np.nanmean(IoUs) * 100\n",
    "        print(\"Get predict image.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            image_path = os.path.join(pred_dir, image_id + \".png\")\n",
    "            image = Image.open(image_path)\n",
    "            result = Image.fromarray(np.uint8((1-np.array(image))*255))\n",
    "            result.save(image_path)\n",
    "    \n",
    "GetResult = GetTestResult(model, input_shape, num_classes, test_lines, data_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)\n",
    "GetResult.get_result(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 348/348 [06:16<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-39602163b541>:94: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(hist, np.int), IoUs, PA_Recall, Precision\n",
      "\r",
      "  0%|                                                                                          | 0/348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> mIoU: 87.16; mPA: 91.91; Accuracy: 95.06\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 348/348 [00:53<00:00,  6.46it/s]\n"
     ]
    }
   ],
   "source": [
    "test_lines = total_lines[1998+348:]\n",
    "# test_dataset = UnetDataset(test_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "# test_sampler = None\n",
    "# gen_test = DataLoader(test_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                                     drop_last = True, collate_fn = unet_dataset_collate, sampler=test_sampler)\n",
    "class GetTestResult():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(GetTestResult, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "#         if self.eval_flag:\n",
    "#             with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "#                 f.write(str(0))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def get_result(self, model_eval):\n",
    "        self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "        gt_dir      = os.path.join(self.dataset_path, \"Labels\")\n",
    "#         pred_dir    = '/home/ubuntu/user_space/detection-results'\n",
    "        pred_dir = 'd://swin-tran-total-line3/detetion-results25'\n",
    "        if not os.path.exists(self.miou_out_path):\n",
    "            os.makedirs(self.miou_out_path)\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "        print(\"Get miou.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            #-------------------------------#\n",
    "            #   从文件中读取图像\n",
    "            #-------------------------------#\n",
    "            image_path  = os.path.join(self.dataset_path, \"Images/\"+image_id+\".jpg\")\n",
    "            image       = Image.open(image_path)\n",
    "            #------------------------------#\n",
    "            #   获得预测txt\n",
    "            #------------------------------#\n",
    "            image       = self.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "\n",
    "        print(\"Calculate miou.\")\n",
    "        _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "        temp_miou = np.nanmean(IoUs) * 100\n",
    "        print(\"Get predict image.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            image_path = os.path.join(pred_dir, image_id + \".png\")\n",
    "            image = Image.open(image_path)\n",
    "            result = Image.fromarray(np.uint8((1-np.array(image))*255))\n",
    "            result.save(image_path)\n",
    "    \n",
    "GetResult = GetTestResult(model, input_shape, num_classes, test_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)\n",
    "GetResult.get_result(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name          = total_lines[1998+360].split()[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape   = [224, 224]\n",
    "image         = Image.open(os.path.join(os.path.join(train_dataset.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "label         = Image.open(os.path.join(os.path.join(train_dataset.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "# jpg, png    = train_dataset.get_random_data(jpg, png, input_shape, random = False)\n",
    "# jpg         = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2,0,1])\n",
    "# png         = np.array(png)\n",
    "image         = eval_callback.get_miou_png(image)\n",
    "result        = Image.fromarray(np.uint8((1-np.array(image))*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [06:56<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 82.76; mPA: 91.35; Accuracy: 92.0\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [01:59<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
