{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from functools import partial\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.signal\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mmcv\n",
    "# from mmcv.fileio import FileClient\n",
    "# # from mmcv.fileio import load as load_file\n",
    "# from mmcv.parallel import is_module_wrapper\n",
    "# from mmcv.utils import mkdir_or_exist\n",
    "# from mmcv.runner import get_dist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # 利用1x1卷积下降通道数\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        # 利用3x3卷积进行特征提取\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        # 利用1x1卷积上升通道数\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        #-----------------------------------------------------------#\n",
    "        #   假设输入图像为600,600,3\n",
    "        #   当我们使用resnet50的时候\n",
    "        #-----------------------------------------------------------#\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        # 600,600,3 -> 300,300,64\n",
    "        self.conv1  = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1    = nn.BatchNorm2d(64)\n",
    "        self.relu   = nn.ReLU(inplace=True)\n",
    "        # 300,300,64 -> 150,150,64\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change\n",
    "        # 150,150,64 -> 150,150,256\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        # 150,150,256 -> 75,75,512\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        # 75,75,512 -> 38,38,1024\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        # 38,38,1024 -> 19,19,2048\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                    kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        # x = self.layer1(x)\n",
    "        # x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x       = self.conv1(x)\n",
    "        x       = self.bn1(x)\n",
    "        feat1   = self.relu(x)\n",
    "\n",
    "        x       = self.maxpool(feat1)\n",
    "        feat2   = self.layer1(x)\n",
    "\n",
    "        feat3   = self.layer2(feat2)\n",
    "        feat4   = self.layer3(feat3)\n",
    "        feat5   = self.layer4(feat4)\n",
    "        return [feat1, feat2, feat3, feat4, feat5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backbone.load_state_dict(torch.load('/home/ubuntu/MyFiles/resnet50-19c8e357.pth'))\n",
    "backbone.load_state_dict(torch.load('e://毕业论文/resnet50-19c8e357.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del backbone.avgpool\n",
    "del backbone.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPM(nn.ModuleList):\n",
    "    def __init__(self, pool_sizes, in_channels=768, out_channels=256):\n",
    "        super(PPM, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        for pool_size in pool_sizes:\n",
    "            self.append(\n",
    "                nn.Sequential(\n",
    "                    nn.AdaptiveMaxPool2d(pool_size),\n",
    "                    nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1),\n",
    "                )\n",
    "            )     \n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_puts = []\n",
    "        for ppm in self:\n",
    "            ppm_out = nn.functional.interpolate(ppm(x), size=(x.size(2), x.size(3)), mode='bilinear', align_corners=True)\n",
    "            out_puts.append(ppm_out)\n",
    "        return out_puts\n",
    " \n",
    "    \n",
    "class PPMHEAD(nn.Module):\n",
    "    def __init__(self, in_channels=768, out_channels=256, pool_sizes = [1, 2, 3, 6],num_classes=2):\n",
    "        super(PPMHEAD, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.psp_modules = PPM(self.pool_sizes, self.in_channels, self.out_channels)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels + len(self.pool_sizes)*self.out_channels, self.out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.psp_modules(x)\n",
    "        out.append(x)\n",
    "        out = torch.cat(out, 1)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    " \n",
    "class FPNHEAD(nn.Module):\n",
    "    def __init__(self, channels=2048, out_channels=256):\n",
    "        super(FPNHEAD, self).__init__()\n",
    "        self.PPMHead = PPMHEAD(in_channels=channels, out_channels=out_channels)\n",
    "        \n",
    "        self.Conv_fuse1 = nn.Sequential(\n",
    "            nn.Conv2d(channels//2, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Conv_fuse1_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Conv_fuse2 = nn.Sequential(\n",
    "            nn.Conv2d(channels//4, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )    \n",
    "        self.Conv_fuse2_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.Conv_fuse3 = nn.Sequential(\n",
    "            nn.Conv2d(channels//8, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        self.Conv_fuse3_ = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        self.fuse_all = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*4, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_x1 = nn.Conv2d(out_channels, out_channels, 1)\n",
    " \n",
    "    def forward(self, input_fpn):\n",
    "        # b, 2048,16,16\n",
    "        x1 = self.PPMHead(input_fpn[-1])\n",
    "         #b, 256,32,32\n",
    "        x = nn.functional.interpolate(x1, size=(x1.size(2)*2, x1.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = self.conv_x1(x) + self.Conv_fuse1(input_fpn[-2])  #b,256,32,32\n",
    "        x2 = self.Conv_fuse1_(x)\n",
    "        \n",
    "        x = nn.functional.interpolate(x2, size=(x2.size(2)*2, x2.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = x + self.Conv_fuse2(input_fpn[-3]) #b,256, 64, 64\n",
    "        x3 = self.Conv_fuse2_(x)  \n",
    " \n",
    "        x = nn.functional.interpolate(x3, size=(x3.size(2)*2, x3.size(3)*2),mode='bilinear', align_corners=True)\n",
    "        x = x + self.Conv_fuse3(input_fpn[-4]) #b,256,128,128\n",
    "        x4 = self.Conv_fuse3_(x)\n",
    " \n",
    "        x1 = F.interpolate(x1, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    "        x2 = F.interpolate(x2, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    "        x3 = F.interpolate(x3, x4.size()[-2:],mode='bilinear', align_corners=True)\n",
    " \n",
    "        x = self.fuse_all(torch.cat([x1, x2, x3, x4], 1)) #b,256*4,128,128\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class UPerNet(nn.Module):\n",
    "    def __init__(self, num_classes, backbone):\n",
    "        super(UPerNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.in_channels = 2048\n",
    "        self.channels = 256\n",
    "        self.decoder = FPNHEAD()\n",
    "        self.cls_seg = nn.Sequential(\n",
    "            nn.Conv2d(self.channels, self.num_classes, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        x = nn.functional.interpolate(x, size=(x.size(2)*4, x.size(3)*4),mode='bilinear', align_corners=True)\n",
    "        x = self.cls_seg(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_hist(a, b, n):\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    #   a是转化成一维数组的标签，形状(H×W,)；b是转化成一维数组的预测结果，形状(H×W,)\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    k = (a >= 0) & (a < n)\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    #   np.bincount计算了从0到n**2-1这n**2个数中每个数出现的次数，返回值形状(n, n)\n",
    "    #   返回中，写对角线上的为分类正确的像素点\n",
    "    #--------------------------------------------------------------------------------#\n",
    "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)  \n",
    "\n",
    "def per_class_iu(hist):\n",
    "    return np.diag(hist) / np.maximum((hist.sum(1) + hist.sum(0) - np.diag(hist)), 1) \n",
    "\n",
    "def per_class_PA_Recall(hist):\n",
    "    return np.diag(hist) / np.maximum(hist.sum(1), 1) \n",
    "\n",
    "def per_class_Precision(hist):\n",
    "    return np.diag(hist) / np.maximum(hist.sum(0), 1) \n",
    "\n",
    "def per_Accuracy(hist):\n",
    "    return np.sum(np.diag(hist)) / np.maximum(np.sum(hist), 1) \n",
    "\n",
    "def compute_mIoU(gt_dir, pred_dir, png_name_list, num_classes, name_classes=None):  \n",
    "    print('Num classes', num_classes)  \n",
    "    #-----------------------------------------#\n",
    "    #   创建一个全是0的矩阵，是一个混淆矩阵\n",
    "    #-----------------------------------------#\n",
    "    hist = np.zeros((num_classes, num_classes))  #2*2\n",
    "    \n",
    "    #------------------------------------------------#\n",
    "    #   获得验证集标签路径列表，方便直接读取\n",
    "    #   获得验证集图像分割结果路径列表，方便直接读取\n",
    "    #------------------------------------------------#\n",
    "    gt_imgs     = [os.path.join(gt_dir, x + \"_segmentation.png\") for x in png_name_list]  \n",
    "    pred_imgs   = [os.path.join(pred_dir, x + \".png\") for x in png_name_list]  \n",
    "\n",
    "    #------------------------------------------------#\n",
    "    #   读取每一个（图片-标签）对\n",
    "    #------------------------------------------------#\n",
    "    for ind in range(len(gt_imgs)): \n",
    "        #------------------------------------------------#\n",
    "        #   读取一张图像分割结果，转化成numpy数组\n",
    "        #------------------------------------------------#\n",
    "        pred = np.array(Image.open(pred_imgs[ind]))  #0为   1为背景\n",
    "        #------------------------------------------------#\n",
    "        #   读取一张对应的标签，转化成numpy数组\n",
    "        #------------------------------------------------#\n",
    "        png = np.array(Image.open(gt_imgs[ind])) #0为背景， 255为目标\n",
    "        label  = np.zeros_like(png)\n",
    "        label[png <= 127.5] = 1\n",
    "\n",
    "        # 如果图像分割结果与标签的大小不一样，这张图片就不计算\n",
    "        if len(label.flatten()) != len(pred.flatten()):  \n",
    "            print(\n",
    "                'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}'.format(\n",
    "                    len(label.flatten()), len(pred.flatten()), gt_imgs[ind],\n",
    "                    pred_imgs[ind]))\n",
    "            continue\n",
    "\n",
    "        #------------------------------------------------#\n",
    "        #   对一张图片计算21×21的hist矩阵，并累加\n",
    "        #------------------------------------------------#\n",
    "        hist += fast_hist(label.flatten(), pred.flatten(), num_classes)  \n",
    "        # 每计算10张就输出一下目前已计算的图片中所有类别平均的mIoU值\n",
    "        if name_classes is not None and ind > 0 and ind % 10 == 0: \n",
    "            print('{:d} / {:d}: mIou-{:0.2f}%; mPA-{:0.2f}%; Accuracy-{:0.2f}%'.format(\n",
    "                    ind, \n",
    "                    len(gt_imgs),\n",
    "                    100 * np.nanmean(per_class_iu(hist)),\n",
    "                    100 * np.nanmean(per_class_PA_Recall(hist)),\n",
    "                    100 * per_Accuracy(hist)\n",
    "                )\n",
    "            )\n",
    "    #------------------------------------------------#\n",
    "    #   计算所有验证集图片的逐类别mIoU值\n",
    "    #------------------------------------------------#\n",
    "    IoUs        = per_class_iu(hist)\n",
    "    PA_Recall   = per_class_PA_Recall(hist)\n",
    "    Precision   = per_class_Precision(hist)\n",
    "    #------------------------------------------------#\n",
    "    #   逐类别输出一下mIoU值\n",
    "    #------------------------------------------------#\n",
    "    if name_classes is not None:\n",
    "        for ind_class in range(num_classes):\n",
    "            print('===>' + name_classes[ind_class] + ':\\tIou-' + str(round(IoUs[ind_class] * 100, 2)) \\\n",
    "                + '; Recall (equal to the PA)-' + str(round(PA_Recall[ind_class] * 100, 2))+ '; Precision-' + str(round(Precision[ind_class] * 100, 2)))\n",
    "\n",
    "    #-----------------------------------------------------------------#\n",
    "    #   在所有验证集图像上求所有类别平均的mIoU值，计算时忽略NaN值\n",
    "    #-----------------------------------------------------------------#\n",
    "    print('===> mIoU: ' + str(round(np.nanmean(IoUs) * 100, 2)) + '; mPA: ' + str(round(np.nanmean(PA_Recall) * 100, 2)) + '; Accuracy: ' + str(round(per_Accuracy(hist) * 100, 2)))  \n",
    "    hist = np.array(hist)\n",
    "    return np.array(hist, np.int), IoUs, PA_Recall, Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory():\n",
    "    def __init__(self, log_dir, model, input_shape, val_loss_flag=True):\n",
    "        self.log_dir        = log_dir\n",
    "        self.val_loss_flag  = val_loss_flag\n",
    "\n",
    "        self.losses         = []\n",
    "        if self.val_loss_flag:\n",
    "            self.val_loss   = []\n",
    "        \n",
    "        os.makedirs(self.log_dir)\n",
    "        self.writer     = SummaryWriter(self.log_dir)\n",
    "        try:\n",
    "            dummy_input     = torch.randn(2, 3, input_shape[0], input_shape[1])\n",
    "            self.writer.add_graph(model, dummy_input)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def append_loss(self, epoch, loss, val_loss = None):\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        self.losses.append(loss)\n",
    "        if self.val_loss_flag:\n",
    "            self.val_loss.append(val_loss)\n",
    "        \n",
    "        with open(os.path.join(self.log_dir, \"epoch_loss.txt\"), 'a') as f:\n",
    "            f.write(str(loss))\n",
    "            f.write(\"\\n\")\n",
    "        if self.val_loss_flag:\n",
    "            with open(os.path.join(self.log_dir, \"epoch_val_loss.txt\"), 'a') as f:\n",
    "                f.write(str(val_loss))\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "        self.writer.add_scalar('loss', loss, epoch)\n",
    "        if self.val_loss_flag:\n",
    "            self.writer.add_scalar('val_loss', val_loss, epoch)\n",
    "            \n",
    "        self.loss_plot()\n",
    "\n",
    "    def loss_plot(self):\n",
    "        iters = range(len(self.losses))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses, 'red', linewidth = 2, label='train loss')\n",
    "        if self.val_loss_flag:\n",
    "            plt.plot(iters, self.val_loss, 'coral', linewidth = 2, label='val loss')\n",
    "            \n",
    "        try:\n",
    "            if len(self.losses) < 25:\n",
    "                num = 5\n",
    "            else:\n",
    "                num = 15\n",
    "            \n",
    "            plt.plot(iters, scipy.signal.savgol_filter(self.losses, num, 3), 'green', linestyle = '--', linewidth = 2, label='smooth train loss')\n",
    "            if self.val_loss_flag:\n",
    "                plt.plot(iters, scipy.signal.savgol_filter(self.val_loss, num, 3), '#8B4513', linestyle = '--', linewidth = 2, label='smooth val loss')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.savefig(os.path.join(self.log_dir, \"epoch_loss.png\"))\n",
    "\n",
    "        plt.cla()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "class EvalCallback():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(EvalCallback, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "        if self.eval_flag:\n",
    "            with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "                f.write(str(0))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def on_epoch_end(self, epoch, model_eval):\n",
    "        if epoch % self.period == 0 and self.eval_flag:\n",
    "            self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "            gt_dir      = os.path.join(self.dataset_path, \"Labels\")\n",
    "            pred_dir    = os.path.join(self.miou_out_path, 'detection-results')\n",
    "            if not os.path.exists(self.miou_out_path):\n",
    "                os.makedirs(self.miou_out_path)\n",
    "            if not os.path.exists(pred_dir):\n",
    "                os.makedirs(pred_dir)\n",
    "            print(\"Get miou.\")\n",
    "            for image_id in tqdm(self.image_ids):\n",
    "                #-------------------------------#\n",
    "                #   从文件中读取图像\n",
    "                #-------------------------------#\n",
    "                image_path  = os.path.join(self.dataset_path, \"Images/\"+image_id+\".jpg\")\n",
    "                image       = Image.open(image_path)\n",
    "                #------------------------------#\n",
    "                #   获得预测txt\n",
    "                #------------------------------#\n",
    "                image       = self.get_miou_png(image)\n",
    "                image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "                        \n",
    "            print(\"Calculate miou.\")\n",
    "            _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "            temp_miou = np.nanmean(IoUs) * 100\n",
    "\n",
    "            self.mious.append(temp_miou)\n",
    "            self.epoches.append(epoch)\n",
    "\n",
    "            with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "                f.write(str(temp_miou))\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(self.epoches, self.mious, 'red', linewidth = 2, label='train miou')\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Miou')\n",
    "            plt.title('A Miou Curve')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "\n",
    "            plt.savefig(os.path.join(self.log_dir, \"epoch_miou.png\"))\n",
    "            plt.cla()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "            print(\"Get miou done.\")\n",
    "            shutil.rmtree(self.miou_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtColor(image):\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
    "        return image \n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image \n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def resize_image(image, size):\n",
    "    iw, ih  = image.size\n",
    "    w, h    = size\n",
    "\n",
    "    scale   = min(w/iw, h/ih)\n",
    "    nw      = int(iw*scale)\n",
    "    nh      = int(ih*scale)\n",
    "\n",
    "    image   = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, nw, nh\n",
    "\n",
    "class UnetDataset(Dataset):\n",
    "    def __init__(self, annotation_lines, input_shape, num_classes, train, dataset_path):\n",
    "        super(UnetDataset, self).__init__()\n",
    "        self.annotation_lines   = annotation_lines\n",
    "        self.length             = len(annotation_lines)\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.train              = train\n",
    "        self.dataset_path       = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annotation_line = self.annotation_lines[index]\n",
    "        name            = annotation_line.split()[0][:-4]\n",
    "\n",
    "        #-------------------------------#\n",
    "        #   从文件中读取图像\n",
    "        #-------------------------------#\n",
    "        jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "        png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "        #-------------------------------#\n",
    "        #   数据增强\n",
    "        #-------------------------------#\n",
    "#         jpg, png    = self.get_random_data(jpg, png, self.input_shape, random = self.train)\n",
    "        jpg, png    = self.get_random_data(jpg, png, self.input_shape, random = False)\n",
    "\n",
    "        jpg         = np.array(jpg)\n",
    "        jpg         = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2,0,1])\n",
    "        png         = np.array(png)\n",
    "        #-------------------------------------------------------#\n",
    "        #   这里的标签处理方式和普通voc的处理方式不同\n",
    "        #   将小于127.5的像素点设置为目标像素点。\n",
    "        #-------------------------------------------------------#\n",
    "        modify_png  = np.zeros_like(png)\n",
    "        modify_png[png <= 127.5] = 1\n",
    "        seg_labels  = modify_png\n",
    "        seg_labels  = np.eye(self.num_classes + 1)[seg_labels.reshape([-1])]\n",
    "        seg_labels  = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes + 1))\n",
    "#         seg_labels  = np.eye(self.num_classes)[seg_labels.reshape([-1])]\n",
    "#         seg_labels  = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes))\n",
    "\n",
    "        return jpg, modify_png, seg_labels\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.3, random=True):\n",
    "        image   = cvtColor(image)\n",
    "        label   = Image.fromarray(np.array(label))\n",
    "        #------------------------------#\n",
    "        #   获得图像的高宽与目标高宽\n",
    "        #------------------------------#\n",
    "        iw, ih  = image.size\n",
    "        h, w    = input_shape\n",
    "\n",
    "        if not random:\n",
    "            iw, ih  = image.size\n",
    "            scale   = min(w/iw, h/ih)\n",
    "            nw      = int(iw*scale)\n",
    "            nh      = int(ih*scale)\n",
    "\n",
    "            image       = image.resize((nw,nh), Image.BICUBIC)\n",
    "            new_image   = Image.new('RGB', [w, h], (128,128,128))\n",
    "            new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "            label       = label.resize((nw,nh), Image.NEAREST)\n",
    "            new_label   = Image.new('L', [w, h], (0))\n",
    "            new_label.paste(label, ((w-nw)//2, (h-nh)//2))\n",
    "            return new_image, new_label\n",
    "\n",
    "        #------------------------------------------#\n",
    "        #   对图像进行缩放并且进行长和宽的扭曲\n",
    "        #------------------------------------------#\n",
    "        new_ar = iw/ih * self.rand(1-jitter,1+jitter) / self.rand(1-jitter,1+jitter)\n",
    "        scale = self.rand(0.25, 2)\n",
    "        if new_ar < 1:\n",
    "            nh = int(scale*h)\n",
    "            nw = int(nh*new_ar)\n",
    "        else:\n",
    "            nw = int(scale*w)\n",
    "            nh = int(nw/new_ar)\n",
    "        image = image.resize((nw,nh), Image.BICUBIC)\n",
    "        label = label.resize((nw,nh), Image.NEAREST)\n",
    "        \n",
    "        #------------------------------------------#\n",
    "        #   翻转图像\n",
    "        #------------------------------------------#\n",
    "        flip = self.rand()<.5\n",
    "        if flip: \n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        #------------------------------------------#\n",
    "        #   将图像多余的部分加上灰条\n",
    "        #------------------------------------------#\n",
    "        dx = int(self.rand(0, w-nw))\n",
    "        dy = int(self.rand(0, h-nh))\n",
    "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "        new_label = Image.new('L', (w,h), (0))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        new_label.paste(label, (dx, dy))\n",
    "        image = new_image\n",
    "        label = new_label\n",
    "        image = np.array(image)\n",
    "        image_data      = np.array(image, np.uint8)\n",
    "        #---------------------------------#\n",
    "        #   对图像进行色域变换\n",
    "        #   计算色域变换的参数\n",
    "        #---------------------------------#\n",
    "        r               = np.random.uniform(-1, 1, 3) * [hue, sat, val] + 1\n",
    "        #---------------------------------#\n",
    "        #   将图像转到HSV上\n",
    "        #---------------------------------#\n",
    "        hue, sat, val   = cv2.split(cv2.cvtColor(image_data, cv2.COLOR_RGB2HSV))\n",
    "        dtype           = image_data.dtype\n",
    "        #---------------------------------#\n",
    "        #   应用变换\n",
    "        #---------------------------------#\n",
    "        x       = np.arange(0, 256, dtype=r.dtype)\n",
    "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
    "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
    "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
    "\n",
    "        image_data = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
    "        image_data = cv2.cvtColor(image_data, cv2.COLOR_HSV2RGB)\n",
    "        \n",
    "        return image_data, label\n",
    "\n",
    "# DataLoader中collate_fn使用\n",
    "def unet_dataset_collate(batch):\n",
    "    images      = []\n",
    "    pngs        = []\n",
    "    seg_labels  = []\n",
    "    for img, png, labels in batch:\n",
    "        images.append(img)\n",
    "        pngs.append(png)\n",
    "        seg_labels.append(labels)\n",
    "    images      = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
    "    pngs        = torch.from_numpy(np.array(pngs)).long() #batchsize, 512,512,    1表示背景\n",
    "    seg_labels  = torch.from_numpy(np.array(seg_labels)).type(torch.FloatTensor)#batchsize, 512,512, num_classes+1\n",
    "    return images, pngs, seg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda = True\n",
    "Cuda = False\n",
    "distributed     = False\n",
    "sync_bn         = False\n",
    "fp16            = False\n",
    "num_classes = 2\n",
    "pretrained  = True\n",
    "model_path  = \"\"\n",
    "input_shape = [224, 224]\n",
    "Init_Epoch          = 0\n",
    "Freeze_Epoch        = 25\n",
    "Freeze_batch_size   = 2\n",
    "UnFreeze_Epoch      = 50\n",
    "Unfreeze_batch_size = 2\n",
    "Freeze_Train        = True\n",
    "Init_lr             = 1e-4\n",
    "Min_lr              = Init_lr * 0.01\n",
    "optimizer_type      = \"adam\"\n",
    "momentum            = 0.9\n",
    "weight_decay        = 0\n",
    "lr_decay_type       = 'step'\n",
    "save_period         = 5\n",
    "save_dir            = 'logs'\n",
    "eval_flag           = True\n",
    "eval_period         = 5\n",
    "# VOCdevkit_path  = '/home/ubuntu/MyFiles/Skin_Datasets/train'\n",
    "VOCdevkit_path  = 'e://毕业论文/Skin_Datasets/train'\n",
    "dice_loss       = True\n",
    "focal_loss      = False\n",
    "cls_weights     = np.ones([num_classes], np.float32)\n",
    "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank      = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_lines = os.listdir('/home/ubuntu/MyFiles/Skin_Datasets/train/Images')[1:-1]\n",
    "# len(total_lines), (2694-1998)/2\n",
    "# # train_lines = total_lines[:1998]\n",
    "# # val_lines = total_lines[1998:1998+348]\n",
    "# train_lines = total_lines[:1998]\n",
    "# val_lines = total_lines[1998:1998+348]\n",
    "# num_train   = len(train_lines)\n",
    "# num_val     = len(val_lines)\n",
    "with open('total_lines_3.txt','r')as f:\n",
    "    total_lines = f.readlines()\n",
    "total_lines = [line.strip() for line in total_lines]\n",
    "# train_lines = total_lines[:1998]\n",
    "# val_lines = total_lines[1998:1998+348]\n",
    "train_lines = total_lines[:1998]\n",
    "val_lines = total_lines[1998:1998+348]\n",
    "num_train   = len(train_lines)\n",
    "num_val     = len(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPerNet(num_classes=num_classes,backbone=backbone).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train     = model.train()\n",
    "if Cuda:\n",
    "    model_train = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    model_train = model_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(lr_decay_type, lr, min_lr, total_iters, warmup_iters_ratio = 0.05, warmup_lr_ratio = 0.1, no_aug_iter_ratio = 0.05, step_num = 10):\n",
    "    def yolox_warm_cos_lr(lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter, iters):\n",
    "        if iters <= warmup_total_iters:\n",
    "            # lr = (lr - warmup_lr_start) * iters / float(warmup_total_iters) + warmup_lr_start\n",
    "            lr = (lr - warmup_lr_start) * pow(iters / float(warmup_total_iters), 2) + warmup_lr_start\n",
    "        elif iters >= total_iters - no_aug_iter:\n",
    "            lr = min_lr\n",
    "        else:\n",
    "            lr = min_lr + 0.5 * (lr - min_lr) * (\n",
    "                1.0 + math.cos(math.pi* (iters - warmup_total_iters) / (total_iters - warmup_total_iters - no_aug_iter))\n",
    "            )\n",
    "        return lr\n",
    "\n",
    "    def step_lr(lr, decay_rate, step_size, iters):\n",
    "        if step_size < 1:\n",
    "            raise ValueError(\"step_size must above 1.\")\n",
    "        n       = iters // step_size\n",
    "        out_lr  = lr * decay_rate ** n\n",
    "        return out_lr\n",
    "\n",
    "    if lr_decay_type == \"cos\":\n",
    "        warmup_total_iters  = min(max(warmup_iters_ratio * total_iters, 1), 3)\n",
    "        warmup_lr_start     = max(warmup_lr_ratio * lr, 1e-6)\n",
    "        no_aug_iter         = min(max(no_aug_iter_ratio * total_iters, 1), 15)\n",
    "        func = partial(yolox_warm_cos_lr ,lr, min_lr, total_iters, warmup_total_iters, warmup_lr_start, no_aug_iter)\n",
    "    else:\n",
    "        decay_rate  = (min_lr / lr) ** (1 / (step_num - 1))\n",
    "        step_size   = total_iters / step_num\n",
    "        func = partial(step_lr, lr, decay_rate, step_size)\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnFreeze_flag = False\n",
    "if Freeze_Train:\n",
    "    model.freeze_backbone()\n",
    "batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
    "epoch_step      = num_train // batch_size\n",
    "epoch_step_val  = num_val // batch_size\n",
    "nbs = 16\n",
    "lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "optimizer = {\n",
    "    'adam'  : optim.Adam(model.parameters(), Init_lr_fit, betas = (momentum, 0.999), weight_decay = weight_decay),\n",
    "    'sgd'   : optim.SGD(model.parameters(), Init_lr_fit, momentum = momentum, nesterov=True, weight_decay = weight_decay)\n",
    "}[optimizer_type]\n",
    "lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "train_dataset   = UnetDataset(train_lines, input_shape, num_classes, True, VOCdevkit_path)\n",
    "val_dataset     = UnetDataset(val_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "train_sampler   = None\n",
    "val_sampler     = None\n",
    "shuffle         = True\n",
    "# gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=True,\n",
    "#                             drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "# gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                             drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=False,\n",
    "                            drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=False, \n",
    "                            drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "eval_callback   = EvalCallback(model, input_shape, num_classes, val_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_Loss(inputs, target, cls_weights, num_classes=2):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    temp_target = target.view(-1)  #1为背景\n",
    "\n",
    "    CE_loss  = nn.CrossEntropyLoss(weight=cls_weights, ignore_index=num_classes)(temp_inputs, temp_target)\n",
    "    return CE_loss\n",
    "\n",
    "def Dice_loss(inputs, target, beta=1, smooth = 1e-5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #--------------------------------------------#\n",
    "    #   计算dice loss\n",
    "    #--------------------------------------------#\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    dice_loss = 1 - torch.mean(score)\n",
    "    return dice_loss\n",
    "\n",
    "def f_score(inputs, target, beta=1, smooth = 1e-5, threhold = 0.5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #--------------------------------------------#\n",
    "    #   计算dice系数\n",
    "    #--------------------------------------------#\n",
    "    temp_inputs = torch.gt(temp_inputs, threhold).float()\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    score = torch.mean(score)\n",
    "    return score\n",
    "\n",
    "def fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank=0):\n",
    "    total_loss      = 0\n",
    "    total_f_score   = 0\n",
    "\n",
    "    val_loss        = 0\n",
    "    val_f_score     = 0\n",
    "\n",
    "    if local_rank == 0:\n",
    "        print('Start Train')\n",
    "        pbar = tqdm(total=epoch_step,desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
    "    model_train.train()\n",
    "    for iteration, batch in enumerate(gen):\n",
    "        if iteration >= epoch_step: \n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs    = imgs.cuda(local_rank)\n",
    "                pngs    = pngs.cuda(local_rank)\n",
    "                labels  = labels.cuda(local_rank)\n",
    "                weights = weights.cuda(local_rank)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if not fp16:\n",
    "            #----------------------#\n",
    "            #   前向传播\n",
    "            #----------------------#\n",
    "            outputs = model_train(imgs)\n",
    "            #----------------------#\n",
    "            #   损失计算\n",
    "            #----------------------#\n",
    "#             if focal_loss:\n",
    "#                 loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "#             else:\n",
    "            loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "            if dice_loss:\n",
    "                main_dice = Dice_loss(outputs, labels)\n",
    "                loss      = loss + main_dice\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #-------------------------------#\n",
    "                #   计算f_score\n",
    "                #-------------------------------#\n",
    "                _f_score = f_score(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            from torch.cuda.amp import autocast\n",
    "            with autocast():\n",
    "                #----------------------#\n",
    "                #   前向传播\n",
    "                #----------------------#\n",
    "                outputs = model_train(imgs)\n",
    "                #----------------------#\n",
    "                #   损失计算\n",
    "                #----------------------#\n",
    "                if focal_loss:\n",
    "                    loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "                else:\n",
    "                    loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "                if dice_loss:\n",
    "                    main_dice = Dice_loss(outputs, labels)\n",
    "                    loss      = loss + main_dice\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    #-------------------------------#\n",
    "                    #   计算f_score\n",
    "                    #-------------------------------#\n",
    "                    _f_score = f_score(outputs, labels)\n",
    "\n",
    "            #----------------------#\n",
    "            #   反向传播\n",
    "            #----------------------#\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_loss      += loss.item()\n",
    "        total_f_score   += _f_score.item()\n",
    "        \n",
    "        if local_rank == 0:\n",
    "            pbar.set_postfix(**{'total_loss': total_loss / (iteration + 1), \n",
    "                                'f_score'   : total_f_score / (iteration + 1),\n",
    "                                'lr'        : get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        pbar.close()\n",
    "        print('Finish Train')\n",
    "        print('Start Validation')\n",
    "        pbar = tqdm(total=epoch_step_val, desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)\n",
    "\n",
    "    model_train.eval()\n",
    "    for iteration, batch in enumerate(gen_val):\n",
    "        if iteration >= epoch_step_val:\n",
    "            break\n",
    "        imgs, pngs, labels = batch\n",
    "        with torch.no_grad():\n",
    "            weights = torch.from_numpy(cls_weights)\n",
    "            if cuda:\n",
    "                imgs    = imgs.cuda(local_rank)\n",
    "                pngs    = pngs.cuda(local_rank)\n",
    "                labels  = labels.cuda(local_rank)\n",
    "                weights = weights.cuda(local_rank)\n",
    "\n",
    "            #----------------------#\n",
    "            #   前向传播\n",
    "            #----------------------#\n",
    "            outputs = model_train(imgs)\n",
    "            #----------------------#\n",
    "            #   损失计算\n",
    "            #----------------------#\n",
    "#             if focal_loss:\n",
    "#                 loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "#             else:\n",
    "            loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)\n",
    "\n",
    "            if dice_loss:\n",
    "                main_dice = Dice_loss(outputs, labels)\n",
    "                loss  = loss + main_dice\n",
    "            #-------------------------------#\n",
    "            #   计算f_score\n",
    "            #-------------------------------#\n",
    "            _f_score    = f_score(outputs, labels)\n",
    "\n",
    "            val_loss    += loss.item()\n",
    "            val_f_score += _f_score.item()\n",
    "            \n",
    "        if local_rank == 0:\n",
    "            pbar.set_postfix(**{'val_loss'  : val_loss / (iteration + 1),\n",
    "                                'f_score'   : val_f_score / (iteration + 1),\n",
    "                                'lr'        : get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "            \n",
    "    if local_rank == 0:\n",
    "        pbar.close()\n",
    "        print('Finish Validation')\n",
    "        loss_history.append_loss(epoch + 1, total_loss/ epoch_step, val_loss/ epoch_step_val)\n",
    "        eval_callback.on_epoch_end(epoch + 1, model_train)\n",
    "        print('Epoch:'+ str(epoch+1) + '/' + str(Epoch))\n",
    "        print('Total Loss: %.3f || Val Loss: %.3f ' % (total_loss / epoch_step, val_loss / epoch_step_val))\n",
    "        \n",
    "        #-----------------------------------------------#\n",
    "        #   保存权值\n",
    "        #-----------------------------------------------#\n",
    "        if (epoch + 1) % save_period == 0 or epoch + 1 == Epoch:\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'ep%03d-loss%.3f-val_loss%.3f.pth'%((epoch + 1), total_loss / epoch_step, val_loss / epoch_step_val)))\n",
    "\n",
    "#         if len(loss_history.val_loss) <= 1 or (val_loss / epoch_step_val) <= min(loss_history.val_loss):\n",
    "#             print('Save best model to best_epoch_weights.pth')\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_epoch_weights.pth\"))\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"last_epoch_weights.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 999/999 [08:26<00:00,  1.97it/s, f_score=0.89, lr=0.0001, total_loss=0.284] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 174/174 [05:42<00:00,  1.97s/it, f_score=0.904, lr=0.0001, val_loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:1/50\n",
      "Total Loss: 0.284 || Val Loss: 0.275 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 999/999 [09:29<00:00,  1.75it/s, f_score=0.919, lr=0.0001, total_loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 174/174 [06:01<00:00,  2.08s/it, f_score=0.896, lr=0.0001, val_loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:2/50\n",
      "Total Loss: 0.208 || Val Loss: 0.302 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 999/999 [08:04<00:00,  2.06it/s, f_score=0.927, lr=0.0001, total_loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 174/174 [05:25<00:00,  1.87s/it, f_score=0.911, lr=0.0001, val_loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:3/50\n",
      "Total Loss: 0.182 || Val Loss: 0.258 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 999/999 [07:55<00:00,  2.10it/s, f_score=0.931, lr=0.0001, total_loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 174/174 [05:28<00:00,  1.89s/it, f_score=0.904, lr=0.0001, val_loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:4/50\n",
      "Total Loss: 0.168 || Val Loss: 0.304 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 999/999 [08:27<00:00,  1.97it/s, f_score=0.937, lr=0.0001, total_loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 174/174 [05:34<00:00,  1.92s/it, f_score=0.891, lr=0.0001, val_loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [09:02<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 79.76; mPA: 88.02; Accuracy: 90.27\n",
      "Get miou done.\n",
      "Epoch:5/50\n",
      "Total Loss: 0.155 || Val Loss: 0.339 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 999/999 [08:56<00:00,  1.86it/s, f_score=0.945, lr=5.99e-5, total_loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 174/174 [05:48<00:00,  2.00s/it, f_score=0.874, lr=5.99e-5, val_loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:6/50\n",
      "Total Loss: 0.134 || Val Loss: 0.433 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 999/999 [08:47<00:00,  1.89it/s, f_score=0.946, lr=5.99e-5, total_loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 174/174 [05:39<00:00,  1.95s/it, f_score=0.885, lr=5.99e-5, val_loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:7/50\n",
      "Total Loss: 0.130 || Val Loss: 0.384 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 999/999 [08:47<00:00,  1.89it/s, f_score=0.95, lr=5.99e-5, total_loss=0.122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 174/174 [05:37<00:00,  1.94s/it, f_score=0.881, lr=5.99e-5, val_loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:8/50\n",
      "Total Loss: 0.122 || Val Loss: 0.426 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 999/999 [08:48<00:00,  1.89it/s, f_score=0.952, lr=5.99e-5, total_loss=0.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 174/174 [05:44<00:00,  1.98s/it, f_score=0.872, lr=5.99e-5, val_loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:9/50\n",
      "Total Loss: 0.116 || Val Loss: 0.487 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 999/999 [08:52<00:00,  1.88it/s, f_score=0.954, lr=5.99e-5, total_loss=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 174/174 [05:36<00:00,  1.93s/it, f_score=0.874, lr=5.99e-5, val_loss=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [09:11<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 76.78; mPA: 84.68; Accuracy: 89.07\n",
      "Get miou done.\n",
      "Epoch:10/50\n",
      "Total Loss: 0.109 || Val Loss: 0.503 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 999/999 [08:39<00:00,  1.92it/s, f_score=0.958, lr=3.59e-5, total_loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 174/174 [05:36<00:00,  1.93s/it, f_score=0.888, lr=3.59e-5, val_loss=0.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:11/50\n",
      "Total Loss: 0.101 || Val Loss: 0.460 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 999/999 [08:41<00:00,  1.91it/s, f_score=0.959, lr=3.59e-5, total_loss=0.0971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 174/174 [05:51<00:00,  2.02s/it, f_score=0.889, lr=3.59e-5, val_loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:12/50\n",
      "Total Loss: 0.097 || Val Loss: 0.438 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 999/999 [08:23<00:00,  1.98it/s, f_score=0.96, lr=3.59e-5, total_loss=0.0941] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 174/174 [07:48<00:00,  2.69s/it, f_score=0.892, lr=3.59e-5, val_loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:13/50\n",
      "Total Loss: 0.094 || Val Loss: 0.443 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 999/999 [09:45<00:00,  1.71it/s, f_score=0.962, lr=3.59e-5, total_loss=0.0905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 174/174 [05:39<00:00,  1.95s/it, f_score=0.893, lr=3.59e-5, val_loss=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:14/50\n",
      "Total Loss: 0.090 || Val Loss: 0.442 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 999/999 [08:04<00:00,  2.06it/s, f_score=0.962, lr=3.59e-5, total_loss=0.0898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 174/174 [05:34<00:00,  1.92s/it, f_score=0.903, lr=3.59e-5, val_loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:57<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 81.39; mPA: 88.97; Accuracy: 91.15\n",
      "Get miou done.\n",
      "Epoch:15/50\n",
      "Total Loss: 0.090 || Val Loss: 0.417 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 999/999 [08:30<00:00,  1.96it/s, f_score=0.965, lr=2.15e-5, total_loss=0.0831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 174/174 [05:37<00:00,  1.94s/it, f_score=0.891, lr=2.15e-5, val_loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:16/50\n",
      "Total Loss: 0.083 || Val Loss: 0.462 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 999/999 [08:25<00:00,  1.98it/s, f_score=0.965, lr=2.15e-5, total_loss=0.0825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 174/174 [05:31<00:00,  1.90s/it, f_score=0.892, lr=2.15e-5, val_loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:17/50\n",
      "Total Loss: 0.082 || Val Loss: 0.437 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 999/999 [08:12<00:00,  2.03it/s, f_score=0.966, lr=2.15e-5, total_loss=0.0787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 174/174 [05:36<00:00,  1.93s/it, f_score=0.886, lr=2.15e-5, val_loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:18/50\n",
      "Total Loss: 0.079 || Val Loss: 0.491 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 999/999 [08:10<00:00,  2.04it/s, f_score=0.967, lr=2.15e-5, total_loss=0.0779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 174/174 [05:31<00:00,  1.90s/it, f_score=0.9, lr=2.15e-5, val_loss=0.424]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:19/50\n",
      "Total Loss: 0.078 || Val Loss: 0.424 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 999/999 [08:28<00:00,  1.96it/s, f_score=0.968, lr=2.15e-5, total_loss=0.075] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 174/174 [05:30<00:00,  1.90s/it, f_score=0.885, lr=2.15e-5, val_loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:56<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 79.53; mPA: 87.01; Accuracy: 90.36\n",
      "Get miou done.\n",
      "Epoch:20/50\n",
      "Total Loss: 0.075 || Val Loss: 0.511 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 999/999 [09:03<00:00,  1.84it/s, f_score=0.968, lr=1.29e-5, total_loss=0.0739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 174/174 [05:33<00:00,  1.92s/it, f_score=0.896, lr=1.29e-5, val_loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:21/50\n",
      "Total Loss: 0.074 || Val Loss: 0.478 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 999/999 [08:23<00:00,  1.98it/s, f_score=0.969, lr=1.29e-5, total_loss=0.0722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 174/174 [05:38<00:00,  1.95s/it, f_score=0.898, lr=1.29e-5, val_loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:22/50\n",
      "Total Loss: 0.072 || Val Loss: 0.474 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 999/999 [08:24<00:00,  1.98it/s, f_score=0.969, lr=1.29e-5, total_loss=0.0712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 174/174 [05:32<00:00,  1.91s/it, f_score=0.89, lr=1.29e-5, val_loss=0.52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:23/50\n",
      "Total Loss: 0.071 || Val Loss: 0.520 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 999/999 [08:28<00:00,  1.96it/s, f_score=0.97, lr=1.29e-5, total_loss=0.07]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 174/174 [05:26<00:00,  1.88s/it, f_score=0.891, lr=1.29e-5, val_loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:24/50\n",
      "Total Loss: 0.070 || Val Loss: 0.523 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 999/999 [08:24<00:00,  1.98it/s, f_score=0.97, lr=1.29e-5, total_loss=0.0695] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 174/174 [05:39<00:00,  1.95s/it, f_score=0.895, lr=1.29e-5, val_loss=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:39<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 80.49; mPA: 87.9; Accuracy: 90.8\n",
      "Get miou done.\n",
      "Epoch:25/50\n",
      "Total Loss: 0.070 || Val Loss: 0.484 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 999/999 [08:52<00:00,  1.88it/s, f_score=0.97, lr=7.74e-6, total_loss=0.0713] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 174/174 [05:32<00:00,  1.91s/it, f_score=0.901, lr=7.74e-6, val_loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:26/50\n",
      "Total Loss: 0.071 || Val Loss: 0.439 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 999/999 [09:58<00:00,  1.67it/s, f_score=0.97, lr=7.74e-6, total_loss=0.0703] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 174/174 [05:57<00:00,  2.05s/it, f_score=0.904, lr=7.74e-6, val_loss=0.394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:27/50\n",
      "Total Loss: 0.070 || Val Loss: 0.394 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 999/999 [08:58<00:00,  1.86it/s, f_score=0.971, lr=7.74e-6, total_loss=0.0668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 174/174 [05:31<00:00,  1.91s/it, f_score=0.893, lr=7.74e-6, val_loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:28/50\n",
      "Total Loss: 0.067 || Val Loss: 0.487 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 999/999 [09:03<00:00,  1.84it/s, f_score=0.973, lr=7.74e-6, total_loss=0.0636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 174/174 [05:30<00:00,  1.90s/it, f_score=0.91, lr=7.74e-6, val_loss=0.393] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:29/50\n",
      "Total Loss: 0.064 || Val Loss: 0.393 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 999/999 [09:20<00:00,  1.78it/s, f_score=0.973, lr=7.74e-6, total_loss=0.0624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 174/174 [06:19<00:00,  2.18s/it, f_score=0.891, lr=7.74e-6, val_loss=0.518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:55<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 79.15; mPA: 86.29; Accuracy: 90.3\n",
      "Get miou done.\n",
      "Epoch:30/50\n",
      "Total Loss: 0.062 || Val Loss: 0.518 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 999/999 [09:07<00:00,  1.82it/s, f_score=0.975, lr=4.64e-6, total_loss=0.058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 174/174 [05:23<00:00,  1.86s/it, f_score=0.898, lr=4.64e-6, val_loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:31/50\n",
      "Total Loss: 0.058 || Val Loss: 0.456 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 999/999 [09:15<00:00,  1.80it/s, f_score=0.976, lr=4.64e-6, total_loss=0.0557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 174/174 [05:26<00:00,  1.87s/it, f_score=0.901, lr=4.64e-6, val_loss=0.496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:32/50\n",
      "Total Loss: 0.056 || Val Loss: 0.496 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 999/999 [09:16<00:00,  1.79it/s, f_score=0.977, lr=4.64e-6, total_loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 174/174 [05:14<00:00,  1.80s/it, f_score=0.891, lr=4.64e-6, val_loss=0.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:33/50\n",
      "Total Loss: 0.053 || Val Loss: 0.538 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 999/999 [09:03<00:00,  1.84it/s, f_score=0.977, lr=4.64e-6, total_loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 174/174 [05:30<00:00,  1.90s/it, f_score=0.899, lr=4.64e-6, val_loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:34/50\n",
      "Total Loss: 0.053 || Val Loss: 0.468 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 174/174 [05:29<00:00,  1.89s/it, f_score=0.911, lr=4.64e-6, val_loss=0.383]05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:51<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 82.88; mPA: 89.58; Accuracy: 92.0\n",
      "Get miou done.\n",
      "Epoch:35/50\n",
      "Total Loss: 0.051 || Val Loss: 0.383 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 999/999 [09:06<00:00,  1.83it/s, f_score=0.979, lr=2.78e-6, total_loss=0.0491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 174/174 [05:20<00:00,  1.84s/it, f_score=0.91, lr=2.78e-6, val_loss=0.388] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:36/50\n",
      "Total Loss: 0.049 || Val Loss: 0.388 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 999/999 [08:58<00:00,  1.86it/s, f_score=0.979, lr=2.78e-6, total_loss=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 174/174 [05:23<00:00,  1.86s/it, f_score=0.896, lr=2.78e-6, val_loss=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:37/50\n",
      "Total Loss: 0.048 || Val Loss: 0.482 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 999/999 [08:55<00:00,  1.87it/s, f_score=0.979, lr=2.78e-6, total_loss=0.0468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 174/174 [05:19<00:00,  1.83s/it, f_score=0.909, lr=2.78e-6, val_loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:38/50\n",
      "Total Loss: 0.047 || Val Loss: 0.429 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 999/999 [08:50<00:00,  1.88it/s, f_score=0.98, lr=2.78e-6, total_loss=0.0458] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 174/174 [06:10<00:00,  2.13s/it, f_score=0.898, lr=2.78e-6, val_loss=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:39/50\n",
      "Total Loss: 0.046 || Val Loss: 0.503 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 999/999 [11:25<00:00,  1.46it/s, f_score=0.98, lr=2.78e-6, total_loss=0.0451] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 174/174 [05:19<00:00,  1.84s/it, f_score=0.908, lr=2.78e-6, val_loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:40<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 82.86; mPA: 89.44; Accuracy: 92.01\n",
      "Get miou done.\n",
      "Epoch:40/50\n",
      "Total Loss: 0.045 || Val Loss: 0.410 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 999/999 [08:50<00:00,  1.88it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 174/174 [05:20<00:00,  1.84s/it, f_score=0.906, lr=1.67e-6, val_loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:41/50\n",
      "Total Loss: 0.044 || Val Loss: 0.455 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 999/999 [08:58<00:00,  1.85it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 174/174 [05:23<00:00,  1.86s/it, f_score=0.9, lr=1.67e-6, val_loss=0.496]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:42/50\n",
      "Total Loss: 0.044 || Val Loss: 0.496 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 999/999 [09:12<00:00,  1.81it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 174/174 [05:34<00:00,  1.92s/it, f_score=0.902, lr=1.67e-6, val_loss=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:43/50\n",
      "Total Loss: 0.043 || Val Loss: 0.458 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 999/999 [08:51<00:00,  1.88it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 174/174 [05:19<00:00,  1.84s/it, f_score=0.908, lr=1.67e-6, val_loss=0.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:44/50\n",
      "Total Loss: 0.043 || Val Loss: 0.440 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 999/999 [08:39<00:00,  1.92it/s, f_score=0.981, lr=1.67e-6, total_loss=0.0424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 174/174 [05:19<00:00,  1.84s/it, f_score=0.903, lr=1.67e-6, val_loss=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:39<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 81.66; mPA: 88.44; Accuracy: 91.45\n",
      "Get miou done.\n",
      "Epoch:45/50\n",
      "Total Loss: 0.042 || Val Loss: 0.494 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 999/999 [08:48<00:00,  1.89it/s, f_score=0.981, lr=1e-6, total_loss=0.042] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 174/174 [05:21<00:00,  1.85s/it, f_score=0.906, lr=1e-6, val_loss=0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:46/50\n",
      "Total Loss: 0.042 || Val Loss: 0.461 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 999/999 [08:47<00:00,  1.89it/s, f_score=0.982, lr=1e-6, total_loss=0.0419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 174/174 [05:19<00:00,  1.83s/it, f_score=0.91, lr=1e-6, val_loss=0.444] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:47/50\n",
      "Total Loss: 0.042 || Val Loss: 0.444 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 999/999 [09:09<00:00,  1.82it/s, f_score=0.982, lr=1e-6, total_loss=0.0411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 174/174 [08:42<00:00,  3.00s/it, f_score=0.899, lr=1e-6, val_loss=0.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:48/50\n",
      "Total Loss: 0.041 || Val Loss: 0.527 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 999/999 [08:45<00:00,  1.90it/s, f_score=0.982, lr=1e-6, total_loss=0.0407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 174/174 [05:23<00:00,  1.86s/it, f_score=0.908, lr=1e-6, val_loss=0.429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:49/50\n",
      "Total Loss: 0.041 || Val Loss: 0.429 \n",
      "Start Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 999/999 [08:43<00:00,  1.91it/s, f_score=0.982, lr=1e-6, total_loss=0.0408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Train\n",
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 174/174 [05:22<00:00,  1.85s/it, f_score=0.912, lr=1e-6, val_loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [08:48<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 83.12; mPA: 89.8; Accuracy: 92.11\n",
      "Get miou done.\n",
      "Epoch:50/50\n",
      "Total Loss: 0.041 || Val Loss: 0.378 \n"
     ]
    }
   ],
   "source": [
    "def set_optimizer_lr(optimizer, lr_scheduler_func, epoch):\n",
    "    lr = lr_scheduler_func(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
    "    #---------------------------------------#\n",
    "    #   如果模型有冻结学习部分\n",
    "    #   则解冻，并设置参数\n",
    "    #---------------------------------------#\n",
    "    if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
    "        batch_size = Unfreeze_batch_size\n",
    "\n",
    "        #-------------------------------------------------------------------#\n",
    "        #   判断当前batch_size，自适应调整学习率\n",
    "        #-------------------------------------------------------------------#\n",
    "        lr_limit_max    = 1e-4 if optimizer_type == 'adam' else 1e-1\n",
    "        lr_limit_min    = 1e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "        #---------------------------------------#\n",
    "        #   获得学习率下降的公式\n",
    "        #---------------------------------------#\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "\n",
    "        model.unfreeze_backbone()\n",
    "\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val // batch_size\n",
    "\n",
    "        if epoch_step == 0:\n",
    "            raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n",
    "\n",
    "\n",
    "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, pin_memory=True,\n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=train_sampler)\n",
    "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "                                    drop_last = True, collate_fn = unet_dataset_collate, sampler=val_sampler)\n",
    "\n",
    "        UnFreeze_flag = True\n",
    "\n",
    "    set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "    fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, \n",
    "                epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank)\n",
    "\n",
    "if local_rank == 0:\n",
    "    loss_history.writer.close()\n",
    "#     fit_one_epoch_no_val(model_train, model, loss_history, optimizer, epoch, epoch_step, gen, UnFreeze_Epoch, Cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [06:10<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n",
      "===> mIoU: 82.68; mPA: 89.13; Accuracy: 92.35\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [01:53<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "test_lines = total_lines[1998+348:]\n",
    "# test_dataset = UnetDataset(test_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "# test_sampler = None\n",
    "# gen_test = DataLoader(test_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                                     drop_last = True, collate_fn = unet_dataset_collate, sampler=test_sampler)\n",
    "class GetTestResult():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(GetTestResult, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "#         if self.eval_flag:\n",
    "#             with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "#                 f.write(str(0))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def get_result(self, model_eval):\n",
    "        self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "        gt_dir      = os.path.join(self.dataset_path, \"Labels\")\n",
    "        pred_dir    = '/home/ubuntu/user_space/detection-results'\n",
    "        if not os.path.exists(self.miou_out_path):\n",
    "            os.makedirs(self.miou_out_path)\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "        print(\"Get miou.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            #-------------------------------#\n",
    "            #   从文件中读取图像\n",
    "            #-------------------------------#\n",
    "            image_path  = os.path.join(self.dataset_path, \"Images/\"+image_id+\".jpg\")\n",
    "            image       = Image.open(image_path)\n",
    "            #------------------------------#\n",
    "            #   获得预测txt\n",
    "            #------------------------------#\n",
    "            image       = self.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "\n",
    "        print(\"Calculate miou.\")\n",
    "        _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "        temp_miou = np.nanmean(IoUs) * 100\n",
    "        print(\"Get predict image.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            image_path = os.path.join(pred_dir, image_id + \".png\")\n",
    "            image = Image.open(image_path)\n",
    "            result = Image.fromarray(np.uint8((1-np.array(image))*255))\n",
    "            result.save(image_path)\n",
    "    \n",
    "GetResult = GetTestResult(model, input_shape, num_classes, test_lines, VOCdevkit_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)\n",
    "GetResult.get_result(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name          = total_lines[1998+351].split()[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape   = [224, 224]\n",
    "image         = Image.open(os.path.join(os.path.join(train_dataset.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "label         = Image.open(os.path.join(os.path.join(train_dataset.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "# jpg, png    = train_dataset.get_random_data(jpg, png, input_shape, random = False)\n",
    "# jpg         = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2,0,1])\n",
    "# png         = np.array(png)\n",
    "image         = eval_callback.get_miou_png(image)\n",
    "result        = Image.fromarray(np.uint8((1-np.array(image))*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('e://毕业论文/resnet-upernet-no-augment/ep050-loss0.041-val_loss0.378.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get miou.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [15:30<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate miou.\n",
      "Num classes 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-39602163b541>:94: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(hist, np.int), IoUs, PA_Recall, Precision\n",
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> mIoU: 80.81; mPA: 88.57; Accuracy: 91.14\n",
      "Get predict image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:46<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_lines = total_lines[1998+348:]\n",
    "# test_dataset = UnetDataset(test_lines, input_shape, num_classes, False, VOCdevkit_path)\n",
    "# test_sampler = None\n",
    "# gen_test = DataLoader(test_dataset  , shuffle = shuffle, batch_size = batch_size, pin_memory=True, \n",
    "#                                     drop_last = True, collate_fn = unet_dataset_collate, sampler=test_sampler)\n",
    "data_path = 'D://360Downloads//ISIC2018_Task1-2_Test_Input'\n",
    "test_lines = os.listdir(data_path)[1:-1]\n",
    "class GetTestResult():\n",
    "    def __init__(self, net, input_shape, num_classes, image_ids, dataset_path, log_dir, cuda, \\\n",
    "            miou_out_path=\".temp_miou_out\", eval_flag=True, period=1):\n",
    "        super(GetTestResult, self).__init__()\n",
    "        \n",
    "        self.net                = net\n",
    "        self.input_shape        = input_shape\n",
    "        self.num_classes        = num_classes\n",
    "        self.image_ids          = image_ids\n",
    "        self.dataset_path       = dataset_path\n",
    "        self.log_dir            = log_dir\n",
    "        self.cuda               = cuda\n",
    "        self.miou_out_path      = miou_out_path\n",
    "        self.eval_flag          = eval_flag\n",
    "        self.period             = period\n",
    "        \n",
    "        self.image_ids          = [image_id.split()[0][:-4] for image_id in image_ids]\n",
    "        self.mious      = [0]\n",
    "        self.epoches    = [0]\n",
    "#         if self.eval_flag:\n",
    "#             with open(os.path.join(self.log_dir, \"epoch_miou.txt\"), 'a') as f:\n",
    "#                 f.write(str(0))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "    def get_miou_png(self, image):\n",
    "        #---------------------------------------------------------#\n",
    "        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "        #---------------------------------------------------------#\n",
    "        image       = cvtColor(image)\n",
    "        orininal_h  = np.array(image).shape[0]\n",
    "        orininal_w  = np.array(image).shape[1]\n",
    "        #---------------------------------------------------------#\n",
    "        #   给图像增加灰条，实现不失真的resize\n",
    "        #   也可以直接resize进行识别\n",
    "        #---------------------------------------------------------#\n",
    "        image_data, nw, nh  = resize_image(image, (self.input_shape[1],self.input_shape[0]))\n",
    "        #---------------------------------------------------------#\n",
    "        #   添加上batch_size维度\n",
    "        #---------------------------------------------------------#\n",
    "        image_data = np.array(image_data)\n",
    "        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, np.float32)), (2, 0, 1)), 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(image_data)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "                \n",
    "            #---------------------------------------------------#\n",
    "            #   图片传入网络进行预测\n",
    "            #---------------------------------------------------#\n",
    "            pr = self.net(images)[0]  #num_class, 512, 512\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = F.softmax(pr.permute(1,2,0),dim = -1).cpu().numpy()\n",
    "            #--------------------------------------#\n",
    "            #   将灰条部分截取掉\n",
    "            #--------------------------------------#\n",
    "            pr = pr[int((self.input_shape[0] - nh) // 2) : int((self.input_shape[0] - nh) // 2 + nh), \\\n",
    "                    int((self.input_shape[1] - nw) // 2) : int((self.input_shape[1] - nw) // 2 + nw)]\n",
    "            #---------------------------------------------------#\n",
    "            #   进行图片的resize\n",
    "            #---------------------------------------------------#\n",
    "            pr = cv2.resize(pr, (orininal_w, orininal_h), interpolation = cv2.INTER_LINEAR)\n",
    "            #---------------------------------------------------#\n",
    "            #   取出每一个像素点的种类\n",
    "            #---------------------------------------------------#\n",
    "            pr = pr.argmax(axis=-1)\n",
    "    \n",
    "        image = Image.fromarray(np.uint8(pr))\n",
    "#         image = Image.fromarray(np.uint8(pr*255))\n",
    "        return image\n",
    "    \n",
    "    def get_result(self, model_eval):\n",
    "        self.net    = model_eval\n",
    "#             jpg         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Images\"), name + \".jpg\"))\n",
    "#             png         = Image.open(os.path.join(os.path.join(self.dataset_path, \"Labels\"), name + \"_segmentation.png\"))\n",
    "#             gt_dir      = os.path.join(self.dataset_path, \"VOC2007/SegmentationClass/\")\n",
    "        gt_dir      = 'e://毕业论文/ISIC2018_Task1_Test_GroundTruth'\n",
    "#         pred_dir    = '/home/ubuntu/user_space/detection-results'\n",
    "        pred_dir = 'd://swin-tran-total-line3/resnet-upernet-test-results50'\n",
    "        if not os.path.exists(self.miou_out_path):\n",
    "            os.makedirs(self.miou_out_path)\n",
    "        if not os.path.exists(pred_dir):\n",
    "            os.makedirs(pred_dir)\n",
    "        print(\"Get miou.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            #-------------------------------#\n",
    "            #   从文件中读取图像\n",
    "            #-------------------------------#\n",
    "            image_path  = os.path.join(self.dataset_path, image_id+'.jpg')\n",
    "            image       = Image.open(image_path)\n",
    "            #------------------------------#\n",
    "            #   获得预测txt\n",
    "            #------------------------------#\n",
    "            image       = self.get_miou_png(image)\n",
    "            image.save(os.path.join(pred_dir, image_id + \".png\"))\n",
    "\n",
    "        print(\"Calculate miou.\")\n",
    "        _, IoUs, _, _ = compute_mIoU(gt_dir, pred_dir, self.image_ids, self.num_classes, None)  # 执行计算mIoU的函数\n",
    "        temp_miou = np.nanmean(IoUs) * 100\n",
    "        print(\"Get predict image.\")\n",
    "        for image_id in tqdm(self.image_ids):\n",
    "            image_path = os.path.join(pred_dir, image_id + \".png\")\n",
    "            image = Image.open(image_path)\n",
    "            result = Image.fromarray(np.uint8((1-np.array(image))*255))\n",
    "            result.save(image_path)\n",
    "    \n",
    "GetResult = GetTestResult(model, input_shape, num_classes, test_lines, data_path, log_dir, Cuda, \\\n",
    "                                eval_flag=eval_flag, period=eval_period)\n",
    "GetResult.get_result(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
